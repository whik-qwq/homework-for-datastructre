 












基于ROS的《机器人基础》实验指导手册
（适用于计算机学院各方向）



















计算机科学与技术系


实验一  ROS入门

一、实验目的和要求
1、安装ubunt操作系统；
2、安装ROS并运行roscore；
3、学习使用make编译C++程序；
4、学习linux中静态库和共享库的基本用法；
二、实验内容	
1、安装ubuntu 20.04；
2、安装ROS，并运行roscore；
3、完成简单Hello world程序的编译链接执行；
4、使用make编译Hello world程序；
5、使用make编译生成Linux中静态库和共享库；
6、ROS下编程
7、按实验步骤进行，并截图记录实验过程；
8、完成实验报告并对实验过程进行总结。
三、实验项目
ROS环境下基础编程实验。
四、实验器材和环境
1、硬件环境：PC机；
2、软件环境：windows10、VMware、Ubuntu 20.04；
五、实验步骤
5.1、安装ros
1、安装ubuntu20.04，并配置系统软件源。
安装ubuntu20.04后，首先需要配置Ubuntu系统，允许restricted(不完全的自由软件)、universe(Ubuntu官方不提供支持与补丁，全靠社区支持)、multiverse(非自由软件，完全不提供支持和补丁)这三种软件源。如果没有对系统软件源做过修改，Ubuntu系统安装完毕后会默认允许以上三种软件源。如下图所示。


2、我们采用下面的命令进行一键安装(强烈推荐初学者)：
          $wget http://fishros.com/install -O fishros && . fishros
	   根据出现的菜单，选择要执行的相关操作，就可以进行安装。
	选择1，根据菜单选择需要安装的ros版本。具体如下：

最后出现界面如下：

选择1，安装noetic版本。
安装成功后运行roscore进行检验。

出现上面的界面说明ROS安装成功。


补充材料：
	安装过程需要访问raw.githubusercontent.com，由于网络速度的影响，经常会遇到各种不可预知的问题，故有兴趣的同学自行参照网上的资料进行安装。下面给出具体的安装过程，如果在安装中出现问题上网搜索解决方法。

1、添加密钥：
使用如下命令添加密钥：
$sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654
如果出现错误:
错误:1 http://mirrors.ustc.edu.cn/ros/ubuntu bionic InRelease 
由于没有公钥,前往ros的wiki官网。更新公钥前往http://wiki.ros.org/kinetic/Installation/Ubuntu，找到set up your key，新终端运行更新公钥的指令

2、安装ROS
$sudo  apt-get   install  ros-noetic-desktop-full
	对于ubuntu16.04安装kinectic版本，ubuntu18.04安装kinectic版本melodic，ubuntu20.04安装noetic版本，noetic版本是ROS1的最后一个长期支持版，以后只能使用ROS2了。

3、初始化rosdep
使用以下命令进行初始化和更新：
	$ sudo  rosdep init
	$ rosdep  update	
如果出现错误：

1)修改系统host文件中的ip地址：
	sudo gedit /etc/hosts
2)在文件末尾添加以下配置语句：
	151.101.84.133 raw.githubusercontent.com
3)保存后退出，然后执行：
	sudo rosdep init
4、设置环境变量
ROS安装成功，默认在/opt路径下。在使用之前需要对环境变量进行设置。
	$ echo  "source  /opt/ros/kinetic/setup.bash"  >>  ~/.bashrc
	$source  ~/.bashrc
5、完成安装
	打开终端运行：
$ roscore
5.2、Linux编程基础
1、编写HelloWorld源程序
选择一个工作目录，例如在/home/XXXX/ros/chap2下（XXX表示用户名），先编写最简单的c++程序，该程序输出Hello World:
	// 文件helloWorld.cpp
#include<iostream>
using namespace std;
int main(int argc, char** argv)
{
	cout<<"Hello World!"<<endl;
	return 0;
}

2、简单编译链接
	$g++  helloWorld.cpp
如果没有问题，则生成了一个可执行文件a.out ，然后在命令行上输入./a.out即可执行此程序：
	$./a.out
	Hello World！

3、使用make
	在当前文件夹中建立一个CMakeLists.txt文件，内容如下：
# 声明要求的cmake最低版本
cmake_minimum_required(VERSION 2.8)
# 声明一个cmake工程
project( HelloWorld )
# 添加一个可执行程序
# 语法：add_executable( 程序名源代码文件 )
add_executable( helloWorld  helloWorld.cpp )
然后，在当前目录下建立build目录，调用cmake对该工程进行分析：
$ mkdir build
$ cd build
$cmake  . .
再用make命令对工程进行编译:
$make
编译过程中会输出一个编译进度，如果顺利通过，就可以得到在CMakeList.txt中声明的那个可执行程序helloWorld。执行它：
	$./helloWorld
	Hello World!

4、使用库
	在/home/XXX/chap2目录下输入libAddInt.cpp文件，文件内容如下：
	//This is a lib file libAddInt.cpp
	int add(int a, int b)
	{
	return a+b;
}
CMakeLists.txt文件内容更新如下：
		# 声明要求的cmake最低版本
cmake_minimum_required(VERSION 2.8)	
# 声明一个cmake工程
project( uselib )	
# 添加一个可执行程序
# 语法：add_executable( 程序名源代码文件 )
# add_executable( helloWorld  helloWorld.cpp )
add_library( add libAddInt.cpp )
使用cmake编译整个工程：
	$cd build
	$cmake  ..
	$make
这时在build文件夹中就会生成一个libadd.a文件，这就是我们得到的库。
如果想生成共享库而非静态库，CMakeLists.txt文件内容为：
		# 声明要求的cmake最低版本
cmake_minimum_required(VERSION 2.8)	
# 声明一个cmake工程
project( uselib )	
# 添加一个可执行程序
# 语法：add_executable( 程序名源代码文件 )
# add_executable( helloWorld  helloWorld.cpp )
	add_library( add_shared SHARED libAddInt.cpp )	
此时经过编译会得到libadd_share.so文件。将该文件拷贝到工程主目录中，以便编译时使用。
5、使用库
	在/home/XXX/chap2目录下输入头文件libAddInt.h，内容如下：
	//libAddInt.h
	#ifndef LIBADDINT_H_
	#define LIBADDINT_H_
	int add(int a, int b);
	#endif	
然后编写可执行程序调用这个函数，程序useAdd.cpp内容如下：
//useAdd.cpp
#include"libAddInt.h"
#include<iostream>
using namespace std;
int main()
{
	int x=5,y=7;
    cout<<add(x,y)<<endl;
    return 0;
}
	  CMakeList.txt中的内容如下：
# 声明要求的cmake最低版本
cmake_minimum_required(VERSION 2.8)	
# 声明一个cmake工程
project( uselib )
#CMAKE_SOURCE_DIR为包含最近一个CMakeLists.txt文件所在的文件夹路径。
include_directories(${CMAKE_SOURCE_DIR})    
link_directories(${CMAKE_SOURCE_DIR}/build)
# 添加一个可执行程序
# 语法：add_executable( 程序名源代码文件 )
# add_executable(helloWorld  helloWorld.cpp)
#add_library( add libAddInt.cpp )
#add_library( add_shared SHARED libAddInt.cpp )
add_executable(useAdd useAdd.cpp)
target_link_libraries(useAdd add_shared)
6、运行结果如下：

5.3、ROS下编程
1、开发工具的安装
① 打开ubuntu software；
② 搜索vscode；
③ 点击code图标安装即可；
④ 在终端输入code即可以打开VS Code。

2、ROS1项目开发流程
① 创建项目
$ mkdir  -p  ~/ros1_dev_ws/src
$ cd  ~/ros1_dev_ws
$ catkin_make
$ cd src
$ catkin_create_pkg hello_robot roscpp 
② 编写代码
可以采用gedit或vim编写源程序，在此建议采用code。 在src/hello_robot/src文件夹下添加main.cpp：
#include<ros/ros.h>
int main(int argc, char **argv)
{
	ros::init(argc, argv, "hello_Robot");
	ROS_INFO("Hello Robot");
	return 0;
}
③ 配置依赖
在src/hello_robot文件夹下的CMakeLists.txt文件的末尾添加：
add_executable(hello_robot src/main.cpp)
target_link_libraries(hello_robot ${catkin_LIBRARIES})
④ 编译
$ cd  ~/ros1_dev_ws
$ catkin_make
⑤ 运行
  	$ roscore
$ source ~/ros1_dev_ws/devel/setup.bash
$ rosrun hello_robot hello_robot
输出结果如下：


3、ROS2
① 创建项目
$ mkdir  -p  ~/ros2_dev_ws/src
$ cd  ~/ros2_dev_ws/src
$ ros2 pkg create --build-type ament_cmake hello_robot
② 编写代码
在src文件夹中添加main.cpp:
#include"rclcpp/rclcpp.hpp"
int main(int argc, char **argv)
{
	rclcpp::init(argc, argv);				//ros client for cpp
	rclcpp::Node node("hello_robot");
	RCLCPP_INFO(node.get_logger(), "Hello Robot!");
	rclcpp::shutdown();
	return 0;
}
③ 配置依赖
在src/hello_robot文件夹下的CMakeLists.txt文件的最后添加：
find_package(rclcpp REQUIRED)
add_executable(hello_robot src/main.cpp)
ament_target_dependencies(hello_robot rclcpp)
install(TARGETS
	hello_robot
	DESTINATION lib/${PROJECT_NAME})
在src/hello_robot文件夹下的package.xml文件中<package format="3">...</package>内的<test_depend>ament_lint_common</test_depend>之后添加：
<depend>rclcpp</depend>
④ 编译
$ cd  ~/ros2_dev_ws
$ colcon build
⑤ 运行
$ cd ~/ros2_dev_ws/install
$ source setup.bash
$ ros2 run hello_robot hello_robot
输出结果如下：




实验二ROS基础
一、实验目的和要求
1、了解ROS中的典型例程turtlesim；
2、掌握ROS中C++的基本编程方法；
3、掌握ROS中python的基本编程方法；
二、实验内容	
1、运行ROS例程turtlesim
2、建立话题Publisher和Subscriber
3、建立服务Server和Client
4、编程控制turtlesim中的小海龟走一个圆、正方形和椭圆形。
5、编程实现turtlesim中的小海龟随机行走完成四个形状（直行、左转直行、右转直行、半圆）后，然后再按原路的返回。
6、编程实现turtlesim中的小海龟2跟随小海龟1，当距离小于某个值比如1后就绕着小海龟1旋转。
7、按实验步骤进行，并截图记录实验过程；
8、完成实验报告并对实验过程进行总结。
三、实验项目
ROSC++/python编程。
四、实验器材和环境
1、硬件环境：PC机；
2、软件环境：windows10、VMware、Ubuntu 20.04；
五、实验步骤
5.1、学习turtlesim例程
1）运行roscore
2）控制乌龟运动
$ rosrun  turtlesim  turtlesim_node
	$ rosrun  turtlesim  turtle_teleop_key
	使用按键控制小乌龟在仿真环境中行进。

5.2、建立话题Publisher和Subscriber
1）创建工作空间和功能包
使用如下目录创建工作空间：
$mkdir  -p   ~/catkin_ws/src
创建完成后，在工作空间的根目录下使用catkin_make命令编译这个工作空间。
	$cd  ~/catkin_ws
	$catkin_make
使用source命令运行这些脚本文件，则工作空间中的环境变量可以生效。
	$source  devel/setup.bash
执行如下命令将环境变量加入到配置文件中：
$ echo  “source  ~/catkin_ws/devel/setup.bash”>>~/.bashrc
2）创建功能包
创建一个learning_ communication功能包，该功能包依赖std_msgs、roscpp、rospy等功能包。
其命令如下：
$ cd  ~/catkin_ws/src
$ catkin_create_pkg  learning_communication std_msgs rospy roscpp
然后回到工作空间的根目录下进行编译，并且设置环境变量：
$ cd  ~/catkin_ws
$ catkin_make
$ source  ~/catkin_ws/devel/setup.bash

3）创建Publisher
	编写代码实现一个节点，节点中创建一个Publisher并发布字符串“Hello World”，源码learning_communication/src/talker.cpp的详细内容如下：
	#include<sstream>
#include"ros/ros.h"
#include"std_msgs/String.h"
int main(int argc,char **argv)
{
		ros::init(argc,argv,"talker");		//ROS节点的初始化 
		ros::NodeHandle n;			//创建节点句柄 
		//创建一个Publisher，发布名为chatter的topic，消息类型为std_msgs::String
		ros::Publisher chatter_pub=n.advertise<std_msgs::String>("chatter",1000);
		ros::Rate  loop_rate(10);		//设置循环的频率，单位是Hz
		int count=0;
		while(ros::ok())
		{
			std_msgs::String msg;	//初始化std_msgs::string类型的消息 
			std::stringstream ss;	
			ss<<"helloe world"<<count;
			msg.data=ss.str();
			ROS_INFO("%s",msg.data.c_str());	//发布消息 
			chatter_pub.publish(msg);
			ros::spinOnce();			//循环等待回调函数 
			loop_rate.sleep();			//按照循环频率延时 
			++count;
		}
		return 0;
}

4）创建Subscriber
创建一个Subscriber以订阅Publisher节点发布的“hello world”字符串,listener.cpp源码如下：
	#include"ros/ros.h"
	#include"std_msgs/String.h"
	//接收到订阅的消息后，会进入消息回调函数 
	void chatterCallback(const std_msgs::String::ConstPtr& msg)
	{
		//将接收到的消息打印出来 
		ROS_INFO("I heard:[%s]",msg->data.c_str());
	}
	 int main(int argc,char **argv)
	{
		ros::init(argc,argv,"listener");		//初始化ROS节点 
		ros::NodeHandle n;			//创建节点句柄 
		//创建一个Subscriber，订阅名为chatter的话题，注册回调函数chatterCallback 
		ros::Subscriber sub=n.subscribe("chatter",1000,chatterCallback);
		ros::spin();	//循环等待回调函数 
		return 0;
	}

5）修改CMakefilelists.txt文件
	对功能包learning_communication中src的CMakeLists.txt文件，在文件的最后增加如下内容： 
	include_directories(include  ${catkin_INCLUDE_DIRS})
	add_executable(talker src/talker.cpp)
	target_link_libraries(talker ${catkin_LIBRARIES})
	#add_dependencies(talker ${PROJECT_NAME}_generate_messages_cpp)
	add_executable(listener src/listener.cpp)
	target_link_libraries(listener ${catkin_LIBRARIES})
	#add_dependencies(listener ${PROJECT_NAME}_generate_messages_cpp)

6）编译
	CMakeLists.txt修改完成后，在工作空间的根路径下开始编译： 
		$ cd  ~/catkin_ws 
		$ catkin_make

7）运行Publisher与Subscriber
首先需要在终端中设置环境变量，即
$ source  ./devel/setup.bash
在终端中启动rosscore
$ roscore
在另一个终端启动Publisher
$rosrun  learning_communication  talker
再开启一个终端启动Subscriber
$ rosrun  learning_communication  listener

5.3、建立服务Server和Client
1）定义服务数据
	创建一个定义服务数据类型的srv文件learning_communication/srv/AddTwoInts.srv:
		int64  a
		int64  b
		---
		int64  sum
2）打开package.xml文件，添加一些依赖配置： 
		<build_depend>message_generation</build_depend>
		<exec_depend>message_runtime</exec_depend>
3）打开CMakeLists.txt文件，在合适的位置（在CMakeLists.txt文件中找到find_package语句处和add_service_files语句处，将注释去掉）添加如下配置： 
		find_package(catkin  REQUIRED COMPONENTS
			geometry_msgs 
			roscpp 
			rospy 
			std_msgs 
			message_generation 
		)
		add_service_files(
			FILES
			AddTwoInts.srv
		)
在## Declare ROS messages, services and actions ##下找到#generate_message修改如下
generate_messages(
   DEPENDENCIES
   std_msgs
)
4）执行catkin_make
$ cd ~/catkin_ws
$ catkin_make
如果编译成功，在devel/include/learning_communication文件夹中可以看到AddTwoInts.h等头文件。
5）创建Server
	源码文件learning_communication/src/server.cpp内容如下：
#include  "ros/ros.h"
	#include  "learning_communication/AddTwoInts.h"
	//service回调函数,输入参数req,输出参数res
	bool  add(learning_communication::AddTwoInts::Request  &req,
		learning_communication::AddTwoInts::Response &res)
	{
	       //将输入参数中的请求数据相加，结果放在应答变量中 
	       res.sum=req.a+req.b; 
	       ROS_INFO("request: x=%ld,y=%ld",(long int)req.a,(long int)req.b);
	       ROS_INFO("sending back response:[%ld]",(long int)res.sum);
	       return true;
	}
	int  main(int argc, char **argv)
	{
	    //ROS节点初始化 
	    ros::init(argc,argv, "add_two_ints_server");
		//创建节点句柄 
		ros::NodeHandle  n;			
		//创建一个名为add_two_ints的server，注册回调函数add()
		ros::ServiceServer   service=n.advertiseService("add_two_ints",add);

		//循环等待回调函数 
		ROS_INFO("Ready  to add  two ints: ");
		ros::spin();
		
		return 0;
     }
6）创建Client
	源码learning_communication/src/ client.cpp的内容如下：
	#include<cstdlib>
	#include"ros/ros.h"
	#include "learning_communication/AddTwoInts.h"
	int main(int argc,char **argv)
	{
	      //ROS节点初始化 
	      ros::init(argc,argv,"add_two_ints_client");	
	      //从终端命令行获取两个加数 
if(argc!=3)
		{
		     ROS_INFO("usage:add_two_ints_client X Y");
		     return 1;	
		}
		//创建节点句柄 
		ros::NodeHandle  n;
		
		//创建一个client,请求add_two_int  service
		//service消息类型是learning_communication::AddTwoInts 
		ros::ServiceClient  client=n.serviceClient<learning_communication:: 				AddTwoInts>("add_two_ints");
		//创建learning_communication::AddTwoInts类型的service消息 
		learning_communication::AddTwoInts  srv;
		srv.request.a=atoll(argv[1]);
		srv.request.b=atoll(argv[2]);
		
		//发布service请求，等待加法运算的应答结果 
		if(client.call(srv))
		{
		     ROS_INFO("Sum: %ld",(long int)srv.response.sum);	
		}
		else
		{
		      ROS_ERROR("Failed to call service add_two_ints");
		      return 1;
		}
		return  0;
	}

7）修改CMakelists.txt文件
	编辑CMakeLists.txt文件如下：
include_directories(include  ${catkin_INCLUDE_DIRS})
	add_executable(server src/server.cpp)		
	target_link_libraries(server  ${catkin_LIBRARIES})
	add_executable(client  src/client.cpp)		
	target_link_libraries(client  ${catkin_LIBRARIES})
	

8）编译
	CMakeLists.txt修改完成后，在工作空间的根路径下开始编译： 
		$ cd  ~/catkin_ws 
		$ catkin_make

9）运行Server和Client
首先需要在终端中设置环境变量，即
$ source  ./devel/setup.bash
在终端中启动rosscore
$ roscore
在另一个终端启动server
$rosrun  learning_communication  server
再开启一个终端启动client
$ rosrun  learning_communication  client 5 2

5.4、建立话题Publisher和Subscriber（采用python编程）
1）创建scripts目录
	在包learning_communication目录下创建scripts目录，用来存放python代码。
	$ mkdir scripts
	$ cd scripts
2）编写python源码文件talker.py，内容如下：
	#!/usr/bin/env python
# license removed for brevity
import rospy
from std_msgs.msg import String

def talker():
		pub = rospy.Publisher('chatter', String, queue_size=10)
		rospy.init_node('talker', anonymous=True)
		rate = rospy.Rate(10) # 10hz
		while not rospy.is_shutdown():
			hello_str = "hello world %s" % rospy.get_time()
			rospy.loginfo(hello_str)
			pub.publish(hello_str)
			rate.sleep()

if __name__ == '__main__':
		try:
			talker()
		except rospy.ROSInterruptException:
			pass
	修改talker.py的文件属性：
	$ chmod +x talker.py
3）编写python源码文件listener.py，内容如下：
	#!/usr/bin/env python
import rospy
from std_msgs.msg import String

def callback(data):
		rospy.loginfo(rospy.get_caller_id() + "I heard %s", data.data)
 
def listener():

# In ROS, nodes are uniquely named. If two nodes with the same
# name are launched, the previous one is kicked off. The
# anonymous=True flag means that rospy will choose a unique
# name for our 'listener' node so that multiple listeners can
# run simultaneously.
		rospy.init_node('listener', anonymous=True)

		rospy.Subscriber("chatter", String, callback)

		# spin() simply keeps python from exiting until this node is stopped
		rospy.spin()

if __name__ == '__main__':
		listener()
	修改listener.py的文件属性：
	$ chmod +x listener.py
	修改CMakeLists.txt，在文件的末尾增加如下内容：
	catkin_install_python(PROGRAMS scripts/talker.py scripts/listener.py
  		DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION}
)
4）运行代码
	进入工作目录，执行catkin_make
	$ cd ~/catkin_ws
	$ catkin_make
	$ rosrun learning_communication talker.py
	$ rosrun learning_communication listener.py

5.5、建立服务Server和Client（采用python编程）
1）定义服务数据（同5.3.1、5.3.2、5.3.3）
2）创建Server
	源码文件learning_communication /scripts/server.py内容如下：
#!/usr/bin/env python
from __future__ import print_function 
from learning_communication.srv import AddTwoInts,AddTwoIntsResponse
import rospy

def handle_add_two_ints(req):
		print("Returning [%s + %s = %s]"%(req.a, req.b, (req.a + req.b)))
		return AddTwoIntsResponse(req.a + req.b)

def add_two_ints_server():
		rospy.init_node('add_two_ints_server')
		s = rospy.Service('add_two_ints', AddTwoInts, handle_add_two_ints)
		print("Ready to add two ints.")
		rospy.spin()

if __name__ == "__main__":
		add_two_ints_server()
3）创建Client
	源码文件learning_communication /scripts/client.py内容如下：
#!/usr/bin/env python
from __future__ import print_function
import sys
import rospy
from learning_communication.srv import *

def add_two_ints_client(x, y):
		rospy.wait_for_service('add_two_ints')
		try:
			add_two_ints = rospy.ServiceProxy('add_two_ints', AddTwoInts)
			resp1 = add_two_ints(x, y)
			return resp1.sum
		except rospy.ServiceException as e:
			print("Service call failed: %s"%e)

def usage():
		return "%s [x y]"%sys.argv[0]

if __name__ == "__main__":
		if len(sys.argv) == 3:
			x = int(sys.argv[1])
			y = int(sys.argv[2])
		else:
			print(usage())
			sys.exit(1)
		print("Requesting %s+%s"%(x, y))
		print("%s + %s = %s"%(x, y, add_two_ints_client(x, y)))
4）修改文件属性
	$ chmod +x server.py
	$ chmod +x client.py
5）修改CMakeLists.txt
catkin_install_python(PROGRAMS scripts/talker.py scripts/listener.py scripts/server.py scripts/client.py  DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION}
)
6）运行
	开启一个终端，运行
	$ rosrun learning_communication server.py
	在打开一个终端，运行
	$ rosrun learning_communication client.py 3 5

5.6、Parameter Service 参数服务器---修改小海龟运行界面的底色

1）创建功能包
$ cd ~/catkin_ws/src
$ catkin_create_pkg  learning_parameter roscpp rospy std_srvs
2）在learning_parameter文件夹下的src文件夹中创建parameters_config.cpp文件，代码如下：
#include<string>
#include<ros/ros.h>
#include<std_srvs/Empty.h>
int main(int argc, char** argv)
{
    ros::init(argc, argv, "parameter_config");
    ros::NodeHandle nh;
    int red, green, blue;
    ros::param::get("/turtlesim/background_r", red);
    ros::param::get("/turtlesim/background_g", green);
    ros::param::get("/turtlesim/background_b", blue);
    ROS_INFO("Get Background Color[%d %d %d]", red, green, blue);
    ros::param::set("/turtlesim/background_r", 255);
    ros::param::set("/turtlesim/background_g", 255);
    ros::param::set("/turtlesim/background_b", 255);
    ROS_INFO("Set Background Color[69, 86, 255]");
    ros::param::get("/turtlesim/background_r", red);
    ros::param::get("/turtlesim/background_g", green);
    ros::param::get("/turtlesim/background_b", blue);
    ROS_INFO("Re-get Background Color[%d %d %d]", red, green, blue);
    //刷新背景颜色
    ros::service::waitForService("/clear");
    ros::ServiceClient clear_background = nh.serviceClient<std_srvs::Empty>("/clear");
    std_srvs::Empty srv;
    clear_background.call(srv);
}
3）配置CMakeLists.txt中的编译规则，插入以下内容：
   	add_executable(parameters_config src/parameters_config.cpp)
	target_link_libraries(parameters_config ${catkin_LIBRARIES})
4）编译运行
	$ cd ~/catkin_ws
	$ catkin_make
	启动一个新终端，运行roscore，再启动一个终端，运行
	$ source devel/setup.bash
	$ rosrun learning_parameter parameter_config
	运行结果如下：
5.7、Actionlib 动作库
1）创建功能包
$ cd ~/catkin_ws/src	
$ catkin_create_pkg learning_action actionlib actionlib_msgs roscpp
2）定义action文件，在learning_action文件夹下创建action文件夹，在action文件夹中创建Readbook.action文件，并输入如下内容：
	# Define the goal
	uint32 total_pages
	---
	# Define the result
	bool is_finish
	---
	# Define a feedback message
	uint32 reading_page
3）在package.xml文件中添加功能包依赖，内容如下：
	<build_depend>actionlib</build_depend>
  	<build_depend>actionlib_msgs</build_depend>
	<exec_depend>actionlib</exec_depend>

4）在CMakeLists.txt文件中的对应位置添加以下内容（即“## Declare ROS messages, services and actions ##”部分）：
	add_action_files(
	  	FILES
	  	Readbook.action
	)
	generate_messages(
	      	DEPENDENCIES
	      	actionlib_msgs
	)
5）编译
	执行catkin_make命令，在devel/include/learning_action目录下会自动生成ReadbookAction.h、ReadbookFeedback.h等文件。
6）在learning_action/src目录下创建readbook_client.cpp文件，程序内容如下：
	#include<ros/ros.h>
	#include<actionlib/client/simple_action_client.h>
	#include"learning_action/ReadbookAction.h"

	void doneCb(const actionlib::SimpleClientGoalState &state, const
    		learning_action::ReadbookResultConstPtr &result)
	{
  ROS_INFO("Finish Reading!");
    ros::shutdown();
}
void activeCb()
{
    ROS_INFO("Goal is active! Begin to read.");
}

void feedbackCb(const learning_action::ReadbookFeedbackConstPtr 
 &feedback)
{
    ROS_INFO("Reading page:%d", feedback->reading_page);
}
int main(int argc, char** argv)
{
    ros::init(argc, argv, "readbook_client");
    actionlib::SimpleActionClient<learning_action::ReadbookAction> 
            client("read_book",true);
    client.waitForServer();
    learning_action::ReadbookGoal goal;
    goal.total_pages=10;
    client.sendGoal(goal, &doneCb, &activeCb, &feedbackCb);
    ros::spin(); 
    return 0;
}
7）在learning_action/src目录下创建readbook_server.cpp文件，程序内容如下：
#include<ros/ros.h>
#include<actionlib/server/simple_action_server.h>
#include"learning_action/ReadbookAction.h"

void execute(const learning_action::ReadbookGoalConstPtr &goal,
  		actionlib::SimpleActionServer<learning_action::ReadbookAction> *as)
{
    ros::Rate r(1);
    learning_action::ReadbookFeedback feedback;
    ROS_INFO("Begin to read %d pages.", goal->total_pages);
    for(int i=0; i<goal->total_pages; i++)
    {
        feedback.reading_page = i;
        as->publishFeedback(feedback);
        r.sleep();
    }
    ROS_INFO("All pages is read.");
    as->setSucceeded();
}
int main(int argc, char** argv)
{
    ros::init(argc, argv, "readbook_server");
    ros::NodeHandle nh;
    actionlib::SimpleActionServer<learning_action::ReadbookAction> 
            server(nh, "read_book", boost::bind(&execute, _1, &server), false);
    server.start();
    ros::spin();
    return 0;
}
8）配置CMakeLists.txt中的编译规则，将下面的代码插入到CMakeLists.txt文件中：
add_compile_options(-std=c++11)
add_executable(readbook_client src/readbook_client.cpp)
add_executable(readbook_server src/readbook_server.cpp)
add_dependencies(readbook_client ${${PROJECT_NAME}_EXPORTRED_TARGETS}${catkin_EXPORTED_TARGETS})
add_dependencies(readbook_server ${${PROJECT_NAME}_EXPORTRED_TARGETS}${catkin_EXPORTED_TARGETS})
target_link_libraries(readbook_client ${catkin_LIBRARIES})
target_link_libraries(readbook_server ${catkin_LIBRARIES})
注：千万不能一股脑塞进去，不然编译的时候顺序会出点问题，所以每次贴的时候最好要看看有没有类似的语句，然后贴在它下面。
9）编译运行
	#启动一个终端
	$ roscore
	#启动一个终端
	$ catkin_make
	$ source  devel/setup.bash
	$ rosrun learning_action readbook_client
	#启动一个终端
	$ source devel/setup.bash
	$ rosrun learning_action readbook_server
客户端的运行结果：
	服务器端的运行结果：
5.8、坐标变换（TF）
1）创建功能包
	$ cd  ~/catkin_ws/src
	$ catkin_create_pkg learning_tf roscpp rospy tf turtlesim
2）在learning_tf文件夹下的src文件夹中创建turtle_tf_broadcaster.cpp文件，内容如下：
#include <ros/ros.h>
#include <tf/transform_broadcaster.h>
#include <turtlesim/Pose.h>
std::string turtle_name;
//位姿回调函数
void poseCallback(const turtlesim::PoseConstPtr& msg)
{
    // 创建tf的广播器
    static tf::TransformBroadcaster br;    
    // 初始化tf数据
    tf::Transform transform;
    transform.setOrigin(tf::Vector3(msg->x, msg->y, 0.0));
    //手动计算旋转矩阵太麻烦，可以通过创建Quaternion对象，并通过rpy作为参数，实例化该对象，它会自动根据rpy的三个参数创建旋转矩阵。
    tf::Quaternion q;
    q.setRPY(0, 0, msg->theta);
    transform.setRotation(q);
    // 广播world与海龟坐标系之间的tf数据
    br.sendTransform(tf::StampedTransform(transform, ros::Time::now(), "world", turtle_name));
}
int main(int argc, char** argv)
{
    //初始化ROS节点
    ros::init(argc, argv, "my_tf_broadcaster");
        // 输入参数作为海龟的名字
    if (argc != 2)
    {
        ROS_ERROR("need turtle name as argument");
        return -1;
    }
    turtle_name=argv[1];
    ros::NodeHandle nh;
    // 订阅海龟的位姿话题
    ros::Subscriber sub = nh.subscribe(turtle_name+"/pose", 10, &poseCallback);
    ros::spin();
    return 0;
}
3）在learning_tf文件夹下的src文件夹中创建turtle_tf_listener.cpp文件，内容如下：
#include  <ros/ros.h>
#include <tf/transform_listener.h>
#include <geometry_msgs/Twist.h>
#include <turtlesim/Spawn.h>
int main(int argc, char** argv){
    // 初始化ROS节点
    ros::init(argc, argv, "my_tf_listener");
    // 创建节点句柄
    ros::NodeHandle nh;
    // 请求产生turtle2
    ros::service::waitForService("/spawn");
    ros::ServiceClient add_turtle = nh.serviceClient<turtlesim::Spawn>("/spawn");
    turtlesim::Spawn srv;
    add_turtle.call(srv);
    // 创建发布turtle2速度控制指令的发布者
    ros::Publisher turtle_vel = nh.advertise<geometry_msgs::Twist>("/turtle2/cmd_vel", 10);    
    // 创建tf的监听器
    tf::TransformListener listener;
    ros::Rate rate(10);     
    while(nh.ok()) 
    {
        tf::StampedTransform transform;
        // 获取turtle1与turtle2坐标系之间的tf数据
        try
        {
            //查询是否有这两个坐标系，查询当前时间，如果超过3s则报错
            listener.waitForTransform("/turtle2", "/turtle1", ros::Time(0), ros::Duration(3.0));
            listener.lookupTransform("/turtle2", "/turtle1", ros::Time(0), transform);
        }
        catch(tf::TransformException &ex)
        {
            ROS_ERROR("%s", ex.what());
            ros::Duration(1.0).sleep();
            continue;  
        }
        // 根据turtle1与turtle2坐标系之间的位置关系，发布turtle2的速度控制指令
        geometry_msgs::Twist vel_msg;
        vel_msg.angular.z = 4.0 * atan2(transform.getOrigin().y(), transform.getOrigin().x());
        vel_msg.linear.x = 0.5 * sqrt(pow(transform.getOrigin().x(), 2) + pow(transform.getOrigin().y(), 2));        
        //发布速度
        turtle_vel.publish(vel_msg);
        rate.sleep();        
    }
    return 0;
}
4）配置CmakeLists.txt中的编译规则，将以下内容输入到文件的末尾：
add_executable(turtle_tf_broadcaster src/turtle_tf_broadcaster.cpp)
target_link_libraries(turtle_tf_broadcaster ${catkin_LIBRARIES})
add_executable(turtle_tf_listener src/turtle_tf_listener.cpp)
target_link_libraries(turtle_tf_listener ${catkin_LIBRARIES})

5）编译
	启动一个终端，运行
	$ catkin_make
	$ roscore
6）由于本实验需要启动的进程过多，可以采用launch文件来进行简化。在learning_tf文件夹下创建launch文件夹，并在launch文件夹中创建start_turtle.launch文件，文件内容如下：
<launch>
    <!-- Turtlesim Node-->
    <node pkg="turtlesim" type="turtlesim_node" name="sim"/>
    <node pkg="turtlesim" type="turtle_teleop_key" name="teleop" output="screen"/>
    
<node pkg="learning_tf" type="turtle_tf_broadcaster" args="/turtle1" name="turtle1_tf_broadcaster" />
    <node pkg="learning_tf" type="turtle_tf_broadcaster" args="/turtle2" name="turtle2_tf_broadcaster" />
    <node pkg="learning_tf" type="turtle_tf_listener" name="listener" />
  </launch>
	launch文件可以用来启动多个节点，采用XML格式的文件进行描述。
$ source  ~/catkin_ws/devel/setup.bash
	$ roslaunch learning_tf start_turtle.launch
	
运行结果如下：
5.9、练习
1）自己完成实验内容4、5、6两部分。
2）附加实验：
编程实现turtlesim中两个小海龟一追一逃，通过实验（也可以先理论计算求解）得出追的小海龟与逃的小海龟速度之比的最小值应该为多少时，才能追上。


实验三  ROS机器人建模
一、实验目的和要求
1、掌握ROS下机器人的建模方法；
二、实验内容	
1、完成机器人的URDF模型；
2、完成URDF模型的xacro改进以及添加传感器模型；
3、创建四个轮子的机器人URDF模型，增加物理和碰撞属性，并显示四轮机器人模型（可以对书中的模型修改，也可以自己建立一个机器人模型）； 大致形状如下图所示：

4、按实验步骤进行，并截图记录实验过程；
5、完成实验报告并对实验过程进行总结。
三、实验项目
机器人建模。
四、实验器材和环境
1、硬件环境：PC机；
2、软件环境：windows10、VMware、Ubuntu 20.04；
五、实验步骤
1、创建机器人描述功能包
使用如下命令创建一个新的功能包：
$catkin_create_pkg  mrobot_description urdf xacro roscpp rospy
重新编译并加载环境配置文件setup.bash。
2、创建URDF模型
	输入模型文件mrobot_description/urdf/mrobot_chassis.urdf，其内容如下：
<?xml version="1.0" ?>
<robot name="mrobot_chassis">

    <link name="base_link">
        <visual>
            <origin xyz="0 0 0" rpy="0 0 0" />
            <geometry>
                <cylinder length="0.005" radius="0.13"/>
            </geometry>
            <material name="yellow">
                <color rgba="1 0.4 0 1"/>
            </material>
        </visual>
    </link>

    <joint name="base_left_motor_joint" type="fixed">
        <origin xyz="-0.055 0.075 0" rpy="0 0 0" />        
        <parent link="base_link"/>
        <child link="left_motor"/>
    </joint>

    <link name="left_motor">
        <visual>
            <origin xyz="0 0 0" rpy="1.5707 0 0" />
            <geometry>
                <cylinder radius="0.02" length = "0.08"/>
            </geometry>
            <material name="gray">
                <color rgba="0.75 0.75 0.75 1"/>
            </material>
        </visual>
    </link>

    <joint name="left_wheel_joint" type="continuous">
        <origin xyz="0 0.0485 0" rpy="0 0 0"/>
        <parent link="left_motor"/>
        <child link="left_wheel_link"/>
        <axis xyz="0 1 0"/>
    </joint>

    <link name="left_wheel_link">
        <visual>
            <origin xyz="0 0 0" rpy="1.5707 0 0" />
            <geometry>
                <cylinder radius="0.033" length = "0.017"/>
            </geometry>
            <material name="white">
                <color rgba="1 1 1 0.9"/>
            </material>
        </visual>
    </link>

    <joint name="base_right_motor_joint" type="fixed">
        <origin xyz="-0.055 -0.075 0" rpy="0 0 0" />        
        <parent link="base_link"/>
        <child link="right_motor"/>
    </joint>

    <link name="right_motor">
        <visual>
            <origin xyz="0 0 0" rpy="1.5707 0 0"/>
            <geometry>
                <cylinder radius="0.02" length = "0.08" />
            </geometry>
            <material name="gray">
                <color rgba="0.75 0.75 0.75 1"/>
            </material>
        </visual>
    </link>

    <joint name="right_wheel_joint" type="continuous">
        <origin xyz="0 -0.0485 0" rpy="0 0 0"/>
        <parent link="right_motor"/>
        <child link="right_wheel_link"/>
        <axis xyz="0 1 0"/>
    </joint>

    <link name="right_wheel_link">
        <visual>
            <origin xyz="0 0 0" rpy="1.5707 0 0" />
            <geometry>
                <cylinder radius="0.033" length = "0.017"/>
            </geometry>
            <material name="white">
                <color rgba="1 1 1 0.9"/>
            </material>
        </visual>
    </link>

    <joint name="front_caster_joint" type="fixed">
        <origin xyz="0.1135 0 -0.0165" rpy="0 0 0"/>
        <parent link="base_link"/>
        <child link="front_caster_link"/>
    </joint>

    <link name="front_caster_link">
        <visual>
            <origin xyz="0 0 0" rpy="1.5707 0 0"/>
            <geometry>
                <sphere radius="0.0165" />
            </geometry>
            <material name="black">
                <color rgba="0 0 0 0.95"/>
            </material>
        </visual>
    </link>

</robot>
这个机器人底盘模型有6个link和5个joint。6个link包括1个机器人底板、2个电机、2个驱动轮和1个万向轮；5个joint负责将驱动轮、万向轮、电机安装到底板上，并设置相应的连接方式。

3、在rviz中显示模型
1）安装joint_state_publisher_gui
$ sudo apt-get install ros-kinetic-joint-state-publisher-gui
2）编写launch文件
	输入mrobot_description/launch/display_mrobot_chassis_urdf.launch文件，其内容如下
<launch>
<param name="robot_description" textfile="$(find mrobot_description)/urdf/mrobot_chassis.urdf" />

	<!-- 设置GUI参数，显示关节控制插件 -->
	<param name="use_gui" value="true"/>  
	
	<!-- 运行joint_state_publisher节点，发布机器人的关节状态  -->
<node name="joint_state_publisher_gui" pkg="joint_state_publisher_gui" type="joint_state_publisher_gui" />
	
	<!-- 运行robot_state_publisher节点，发布tf  -->
<node name="robot_state_publisher" pkg="robot_state_publisher" type= "robot_state_publisher" />
	
	<!-- 运行rviz可视化界面 -->
<node name="rviz" pkg="rviz" type="rviz" args="-d $(find mrobot_description)/config/mrobot_urdf.rviz" required="true" />
</launch>
3）运行launch文件
	运行之前，先将附件中"mrobo_description"->"config"文件夹中的内容拷贝到的自己创建的功能包的config（没有该目录则先创建）目录下，打开终端运行该launch文件，可以看到如下的机器人模型：
	$ roslaunch  mrobot_description  display_mrobot_chassis_urdf.launch


将rivz中“Global Option”中的“Fixed Frame”选项改为base_link，然后按下“Add”按钮，从弹出的对话框中选择“RobotModel”添加，就可以看到机器人的模型。此时不仅启动了rviz，而且出现了一个名为“joint_state_ publisher”的UI。通过操作UI对joint进行控制，可以看到rviz中对应的轮子就会开始转动。

4、改进URDF模型
1）将附件中"mrobo_description"->"launch"文件夹的display_mrobot.launch拷贝到自己创建的功能包的launch目录下，再将"mrobo_description"->"urdf"文件夹中的mrobot.urdf.xacro和mrobot_body.urdf.xacro文件拷贝到自己创建的功能包的urdf目录下。
2）仔细阅读改进后的urdf文件和launch文件。
3）显示优化后的模型
	在终端中运行：
	$roslaunch  mrobot_description  display_mrobot.launch
	显示


5、添加摄像头
1）将附件中"mrobo_description"->"launch"文件夹的display_mrobot_with_camera.launch拷贝到自己创建的功能包的launch目录下，再将"mrobo_description"->"urdf"文件夹中的camera.xacro和mrobot_with_camera.urdf.xacro文件拷贝到自己创建的功能包的urdf目录下。
2）对应的模型文件是mrobot_description/urdf/camera.xacro内容如下:
<?xml version="1.0"?>
<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="camera">

    <xacro:macro name="usb_camera" params="prefix:=camera">
        <link name="${prefix}_link">
            <inertial>
                <mass value="0.1" />
                <origin xyz="0 0 0" />
                <inertia ixx="0.01" ixy="0.0" ixz="0.0"
                         iyy="0.01" iyz="0.0"
                         izz="0.01" />
            </inertial>

            <visual>
                <origin xyz=" 0 0 0 " rpy="0 0 0" />
                <geometry>
                    <box size="0.01 0.04 0.04" />
                </geometry>
                <material name="black"/>
            </visual>

            <collision>
                <origin xyz="0.0 0.0 0.0" rpy="0 0 0" />
                <geometry>
                    <box size="0.01 0.04 0.04" />
                </geometry>
            </collision>
        </link>
    </xacro:macro>

</robot>
3）然后创建一个顶层xacro文件，把机器人和摄像头这两个模块拼装在一起。顶层xacro文件mrobot_description/urdf/ mrobot_with_camera.urdf.xacro的内容如下：
<?xml version="1.0"?>
<robot name="mrobot" xmlns:xacro="http://www.ros.org/wiki/xacro">

    <xacro:include filename="$(find mrobot_description)/urdf/mrobot_body.urdf.xacro" />
    <xacro:include filename="$(find mrobot_description)/urdf/camera.xacro" />

    <xacro:property name="camera_offset_x" value="0.1" />
    <xacro:property name="camera_offset_y" value="0" />
    <xacro:property name="camera_offset_z" value="0.02" />

    <!-- MRobot机器人平台-->
    <xacro:mrobot_body/>

    <!-- Camera -->
    <joint name="camera_joint" type="fixed">
        <origin xyz="${camera_offset_x} ${camera_offset_y} ${camera_offset_z}" rpy="0 0 0" />
        <parent link="plate_2_link"/>
        <child link="camera_link"/>
    </joint>

    <xacro:usb_camera prefix="camera"/>

</robot>
4）launch文件是mrobot_description/launch/display_mrobot_with_camera.launch,内容如下：
<launch>
	<arg name="model" default="$(find xacro)/xacro --inorder '$(find 								mrobot_description)/urdf/mrobot_with_camera.urdf.xacro'" />
	<arg name="gui" default="true" />
	<param name="robot_description" command="$(arg model)" />
    <!-- 设置GUI参数，显示关节控制插件 -->
	<param name="use_gui" value="$(arg gui)"/>
<!-- 运行joint_state_publisher节点，发布机器人的关节状态  -->
<node name="joint_state_publisher_gui" pkg="joint_state_publisher_gui" 				  		type="joint_state_publisher_gui" />
	<!-- 运行robot_state_publisher节点，发布tf  -->
	<node name="robot_state_publisher" pkg="robot_state_publisher" 								type="robot_state_publisher" />
    <!-- 运行rviz可视化界面 -->
	<node name="rviz" pkg="rviz" type="rviz" args="-d $(find 										mrobot_description)/config/mrobot.rviz" required="true" />
</launch>
5）运行
	运行如下命令，在rviz中可以查看装有摄像头的机器人模型：
	$ roslaunch  mrobot_description  display_mrobot_with_camera.launch


6、添加Kinect传感器、激光雷达传感器
	将附件中"mrobo_description"->"meshes"文件夹中的文件拷贝到自己创建的功能包meshes（没有该目录则先创建）目录下，其他类似添加摄像头的操作。

7、自己完成实验内容3.



实验四  ROS机器人仿真
一、实验目的和要求
1、掌握ROS下机器人仿真的编程方法；
三、实验内容	
1、实现并阅读ArbotiX和rviz环境机器人的仿真方法；
2、实现并阅读gazebo环境中机器人的仿真方法（显示机器人模型、控制机器人运动、摄像头仿真、kinect仿真、激光雷达仿真）；
3、编程实现四轮机器人在Gazebo环境中走正方形和圆形；
4、将摄像头安装在四轮机器人上，在gazebo环境中拍摄保存一个视频文件。
5、按实验步骤进行，并截图记录实验过程；
6、完成实验报告并对实验过程进行总结。
三、实验项目
机器人建模。
四、实验器材和环境
1、硬件环境：PC机；
2、软件环境：windows10、VMware、Ubuntu 20.04；
五、实验步骤
1、基于ArbotiX和rviz的仿真器
1）安装ArbotiX
	$ sudo  apt-get  install  ros-noetic-arbotix-*
2）将附件中"mrobo_description"->"urdf"文件夹中的两个文件kinect.xacro、mrobot_with_kinect.urdf.xacro拷贝到自己创建的功能包的urdf目录下。
3）将附件中"mrobo_description"->"launch"文件夹中的文件arbotix_mrobot_with_kinect.launch拷贝到自己创建的功能包的launch目录下。
4）将附件中"mrobo_description"->"config"文件夹中的文件fake_mrobot_arbotix.yaml、mrobot_arbotix.rviz拷贝到自己创建的功能包的config目录下。
5）将附件中"mrobo_description"->"meshes"整个文件夹拷贝到自己创建的功能包下。
6）运行仿真环境
运行命令启动环境：
	$ roslaunch  mrobot_description  arbotix_mrobot_with_kinect.launch
	启动成功后可以看到如下图示：




注意：运行出现[ WARN] ：Joint state with name: “base_l_wheel_joint” was received but not found in URDF，原因是在robot描述文件URDF中关节定义出错，找到launch文件使用的URDF描述文件，并将left_wheel_joint修改成base_l_wheel_joint，将right_wheel_joint修改为base_r_wheel_joint。
7）创建功能包mrobot_teleop
$ catkin_create_pkg mrobot_teleop std_msgs roscpp rospy
在mrobot_telepo文件夹下建立scripts、launch目录
$ mkdir scripts launch
将附件中"mrobot_teleop"->"scripts"文件夹中mrobot_teleop.py拷贝到自己创建的scripts目录下，将附件中"mrobo_teleop"->"launch"文件夹中mrobot_teleop.launch拷贝到自己创建的launch目录下，修改CMakeLists.txt文件，添加
catkin_install_python(PROGRAMS scripts/mrobot_teleop.py
  		DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION})
并修改mrobot_teleop.py为可执行。然后回到工作空间目录下执行
$ catkin_make
编译成功，再开启一个终端，运行
$ roslaunch  mrobot_teleop  mrobot_teleop.launch
可以看到如下内容：

根据提示，按下对应的按键可以得到如下机器人运行轨迹：


2、Gazebo仿真
1）安装并运行Gazebo环境
$ sudo apt-get install ros-noetic-gazebo-ros-pkgs ros-noetic-gazebo-ros-control
2）创建功能包mrobot_gazebo
$ catkin_create_pkg mrobot_gazebo std_msgs gazebo_plugins gazebo_ros_control gazebo_ros roscpp rospy
进入工作空间目录，执行：
$ catkin_make
3）将附件中"mrobo_gazebo"->"launch"文件夹、"urdf"文件夹、"world"文件夹拷贝到自己创建的功能包mrobot_gazebo目录下，
4）在Gazebo中显示机器人模型
	$ roslaunch mrobot_gazebo view_mrobot_gazebo.launch

运行Gazebo的时候报下面的警告：


在配置文件中加入如下两行代码：
<odometrySource>world</odometrySource> <!-- 'encoder' instead of 'world' is also possible -->
<publishTf>1</publishTf>
3、摄像头仿真
1）运行仿真环境
	$ roslaunch mrobot_gazebo view_mrobot_with_camera_gazebo.launch
2）查看结果
运行指令
		$ rqt_image_view
		选择仿真摄像头发布的图像话题/camera/image_raw，即可看到如下图所示的图像信息。


4、kinect仿真
	类似camera仿真，运行仿真环境
	$ roslaunch  mrobot_gazebo view_mrobot_with_kinect_gazebo.launch
	查看点云数据命令：	
$ rosrun  rviz  rviz
 

5、激光雷达仿真
类似camera仿真，运行仿真环境
	$ roslaunch  mrobot_gazebo view_mrobot_with_laser_gazebo.launch
	查看激光数据命令：	
$ rosrun  rvize  rviz


6、自己完成实验内容3和4。





实验五  机器人SLAM与自主导航
一、实验目的和要求
1、了解仿真环境下机器人SLAM与自主导航技术；
二、实验内容	
1、完成gmapping相关实验仿真；
2、完成hector-slam实验仿真
3、完成orb_slam实验仿真
4、完成rviz中机器人导航实验仿真
5、实现gazebo中机器人自主导航实验仿真
6、按实验步骤进行，并截图记录实验过程；
7、完成实验报告并对实验过程进行总结。
三、实验项目
机器人SLAM与自主导航仿真。
四、实验器材和环境
1、硬件环境：PC机；
2、软件环境：windows10、VMware、Ubuntu 20.04；
五、实验步骤
1、gmapping
1）安装gmapping环境
$ sudo  apt-get install ros-noetic-gmapping
$ sudo  apt-get install ros-noetic-navigation
2）创建mrobot_navigation功能包
$ catkin_create_pkg mrobot_navigation geometry_msgs move_base_msgs roscpp rospy tf visualization_msgs
回到工作空间目录
$ catkin_make
3）将附件"mrobot_navigation"文件夹中所有子目录拷贝到自己创建的功能包mrobot_navigation目录下。执行
$ catkin_make
4）打开终端，运行
$ cd ~/catkin_ws
$ source devel/setup.bash
$ roslaunch mrobot_gazebo mrobot_laser_nav_gazebo.launch
5）在另一个终端，运行
$ cd ~/catkin_ws
$ source devel/setup.bash
$ roslaunch mrobot_navigation gmapping_demo.launch
6）在第三个终端，运行
$ cd ~/catkin_ws
$ source devel/setup.bash
$ roslaunch mrobot_teleop  mrobot_teleop.launch
7）在终端中使用键盘控制机器人在仿真环境中移动。则rviz中的地图不断更新，并且gmapping会自动校正之前建立的地图和机器人的位置偏差。最终会生成完整的地图如下。

8）保存当前地图
打开新终端，运行
$ rosrun  map_server  map_saver
地图会保存在当前终端所在的目录下，默认命名为map，不仅包含一个map.pgm地图数据文件，还包含一个map.yaml文件。map.pgm地图数据文件可以使用GIMP等软件进行编辑;map.yaml文件是一个关于地图的配置文件。
使用kinect实现SLAM建图方法与上面的激光雷达步骤一致。在启动仿真环境时加载安装kinect的机器人模型即可。

2、hector-slam
1）安装hector-slam环境
$ sudo apt-get install ros-noetic-hector-slam
2）打开终端，运行
$ roslaunch mrobot_gazebo mrobot_laser_nav_gazebo.launch
3）在另一个终端，运行
$ roslaunch mrobot_navigation hector_demo.launch
4）在第三个终端，运行
$ roslaunch mrobot_teleop  mrobot_teleop.launch
5）在终端中使用键盘控制机器人在仿真环境中移动。则rviz中的地图不断更新，并且gmapping会自动校正之前建立的地图和机器人的位置偏差。最终会生成完整的地图如下。

hector_slam与gmapping最大的不同是不需要订阅里程计/odem消息，而是直接使用激光估算里程计信息，因此，当机器人速度较快是会发生打滑现象，导致建图效果出现偏差。	
3、orbslam（选做）
1）ORB_SLAM功能包
	ORB_SLAM需要下载源码并安装，目前最新的版本是ORB_SLAM2。使用如下命令从github上下载：
	$ git clone https://github.com/raulmur/ORB_SLAM2.git
		在编译之前还需要安装相应的依赖包：
	$ sudo apt-get install libboost-all-dev libblas-dev liblapack-dev
	$ sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev
然后还需要下载源码，编译安装一个数学运算库—eigen。登录eigen的官网下载eigen 3.2版本的源码（http://eigen.tuxfamily.org/index.php?title=Main_Page）。		
	下载、解压源码后，在终端中进入源码目录，使用下面的命令完成eigen的编译、安装：
	$ mkdir build
	$ cd build
	$ cmake ..
	$ make
	$ sudo make install
然后还需要下载下载、编译、安装Pangolin：
	$ git clone https://github.com/stevenlovegrove/Pangolin
	$ cd Pangolin
	$ mkdir build
	$ cd build
	$ cmake –DCP11_NO_BOOST=1 ..
	$ make
	$ sudo make install
接下来就可以编译ORB_SLAM2的源码了。进入源码根目录，使用如下命令完成编译安装：
	$ mkdir builde
	$ cd build
	$ cmake –DCPP11_NO_BOOST=1 ..
	$ make
	$ sudo make install
	在终端中设置ORB_SLAM2包的路径，最好将设置放入终端配置文件中：
	$ export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:ORB_SLAM_PATH/ORB_SLAM2/Examples/ROS
接下来就可以编译ORB_SLAM2的源码了。进入源码根目录，使用如下命令完成编译安装：
	$ mkdir builde
	$ cd build
	$ cmake –DCPP11_NO_BOOST=1 ..
	$ make
	$ sudo make install
		在终端中设置ORB_SLAM2包的路径，最好将设置放入终端配置文件中：
	$ export ROS_PACKAGE_PATH=${ROS_PACKAGE_PATH}:ORB_SLAM_PATH/ORB_SLAM2/Examples/ROS
如果./build_ros.sh出现错误：
	CMake Error at /opt/ros/kinetic/share/ros/core/rosbuild/private.cmake:102 (message): [rosbuild] rospack found package "ORB_SLAM2" at "", but the current directory is "/home/nvidia/ORBSLAM2/ORB_SLAM2/Examples/ROS/ORB_SLAM2". You should double-check your ROS_PACKAGE_PATH to ensure that packages are found in the correct precedence order.
	解决办法：输入 sudo ln -s /home/ORB_SLAM2/Examples/ROS/ORB_SLAM2 /opt/ros/kinetic/share/ORB_SLAM2
2）使用数据包实现单目SLAM
	为了测试ORB_SLAM2功能包是否安装成功，并且了解该功能包的使 用方法，使用数据包进行测试。
	使用roscore命令启动ROS系统，然后通过以下命令运行ORB_SLAM2 中的单目SLAM节点：
	$ rosrun ORB_SLAM2 Mono Vocabulary/ORBvoc.txt Examples/ROS/ORB_SLAM2/ Asus.Yaml
启动成功后就可以看到如下图所示的界面，暂时还没有数据产 生，所以界面中没有任何显示：

现在播放数据包：
	$ rosbag play rgbd_dataset_freiburg1_desk.bag /camera/rgb/image_color:=	/camera/image_raw


4、在rviz中机器人导航
1）安装导航功能包
$ sudo apt-get install ros-noetic-navigation
2）在工作空间中创建功能包mrobot_bringup
$ catkin_create_pkg mrobot_bringup roscpp rospy sensor_msgs geometry_msgs tf
3）回到工作空间，编译
$ cd ..
$ catkin_make
4）将附件中mrobot_bringup源码复制到功能包mrobot_bringup对应位置。
5）回到工作空间，执行catkin_make.
6）在工作空间中创建功能包mrobot_navigation
$ catkin_create_pkg mrobot_navigation roscpp rospy move_base_msgs sensor_msgs geometry_msgs tf visualization_msgs
7）将附件中mrobot_navigation源码复制到功能包mrobot_navigation对应位置。
8）回到工作空间，执行catkin_make.
9）开始导航		
	现在，运行启动文件，开始导航之旅： 
		$ roslaunch mrobot_bringup fake_mrobot_with_laser.launch 
		$ roslaunch mrobot_navigation fake_nav_demo.launch 
	如果一切正常，应该可以看到rviz启动并且加载了我们设置的地 图，这里所使用的就是机器人在实际办公室环境下由gmapping建立的 地图。

用鼠标点击菜单栏中的“2D Nav Goal”按钮，这个按钮的功能是 帮助我们设置导航的目标点。将鼠标移动到地图上导航的目标位置， 点击鼠标左键（注意不要放开）。这时，可以在目标位置看到一个绿 色的箭头，因为导航目标点不仅包含机器人的位置信息，还包含机器 人的姿态信息，通过拖动鼠标可以设置导航目标的姿态。

确定目标后，松开鼠标左键，在机器人的当前位置和目标位置之 间马上就可以看到move_base功能包使用全局规划器创建了一条最优路 径。虽然机器人很快就开始移动，但是，由于受机器人物理参数的限 制，机器人不能完全按照最优路径移动，在机器人附近有一条红色的 短线，这就是本地规划器为机器人规划的当前周期最优速度，尽量保 证机器人靠近全局最优路径移动。 机器人在到达目标位置后会旋转到指定的姿态，导航过程结束。


5、在Gazebo中仿真机器人导航 
1）使用如下命令在Gazebo中仿真机器人导航功能了： 
	$ roslaunch mrobot_gazebo mrobot_laser_nav_gazebo.launch 
	$ roslaunch mrobot_navigation fake_nav_cloister_demo.launch 
		第一个launch文件运行成功后会出现Gazebo界面，第二个launch 文件运行成功后会打开rviz界面。启动成功后的界面如图所示。

使用rviz中的工具选择导航目标点， 机器人即可自主导航到目标位置。
2）实时避障
在Gazebo中动态加入一些障碍物，比如在机器人的运动轨迹上加入一个如图所示的障碍物。
机器人运动到障碍物附近时会停止运动，并重新规划路线。 规划成功后，机器人会绕过障碍物继续向目标点运动。 可见，move_base功能包不仅可以实现全局最优路径的规划，同时 可以利用本地路径规划避开出现的障碍物。 



6、自主导航
1）使用如下命令启动系统： 
	$ roslaunch mrobot_gazebo mrobot_laser_nav_gazebo.launch 
	$ roslaunch mrobot_navigation exploring_slam_demo.launch 
	启动成功后可以打开Gazebo和rviz的界面。

接下来无需启动键盘控制节点，而是类似于导航功能的实现，使 用rviz中的2D Nav Goal工具，在rviz中选择一个导航的目标位置。
	确定目标位置后，机器人开始导航移动，同时使用gmapping实现地图的构建。
2）实现自主探索SLAM
运行系统： 
	$ roslaunch mrobot_gazebo mrobot_laser_nav_gazebo.launch 
	$ roslaunch mrobot_navigation exploring_slam_demo.launch 
		启动成功后，运行exploring_slam.py： 
	$ rosrun mrobot_navigation exploring_slam.py 
	Gazebo和rviz中的机器人很快就开始移动了，通过随机产生的目 标点，机器人一边导航避障，一边完成SLAM建图。


备注：
1、move_base Warning: Invalid argument "/map" passed to canTransform argument target_frame的解决方法...
将navigation->config->mrobot文件夹中的global_costmap_params.yaml和local_costmap_params.yaml文件里的头几行去掉“/”，然后重新编译就可以了。





































实验六  Turtlebot3开发环境搭建
一、实验目的和要求
1、搭建机器人turtlebot3的仿真开发环境；
2、基于开源环境学习turtlebot3自主导航的原理与代码；
二、实验内容	
1、下载turtlebot3机器人的相关源码；
2、搭建turtlebot3的仿真开发环境；
3、学习其导航代码；
4、按实验步骤进行，并截图记录实验过程；
5、完成实验报告并对实验过程进行总结。
三、实验项目
turtlebot3仿真开发实践。
四、实验器材和环境
1、硬件环境：PC机；
2、软件环境：windows10、VMware、Ubuntu 20.04；
五、实验步骤
1、下载turtlebot3源码
具体步骤如下：
$ cd ~/catkin_ws/src/
$ git clone https://github.com/ROBOTIS-GIT/turtlebot3_msgs.git
$ git clone https://github.com/ROBOTIS-GIT/turtlebot3.git
$ git clone https://github.com/ROBOTIS-GIT/turtlebot3_simulations
$ cd ~/catkin_ws && catkin_make
$ source devel/setup.bash
$ roslaunch turtlebot3_fake  turtlebot3_fake.launch
运行结果如下：

2、turtlebot3建图实验
1）安装turtlebot3相关的ROS包
$ sudo apt-get install ros-noetic-turtlebot3 ros-noetic-turtlebot3-description ros-noetic-turtlebot3-gazebo ros-noetic-turtlebot3-msgs ros-noetic-turtlebot3-slam ros-noetic-turtlebot3-teleop
2）安装slam相关的ros包
$ sudo apt-get install ros-noetic-slam-karto
3）建图操作
$ export TURTLEBOT3_MODLE=burger  
#除了burger也可以是waffle或waffle_pi，也可保存在bashrc文件里
$ roslaunch turtlebot3_gazebo turtlebot3_world.launch
$ roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
# 建图时要切换到这个终端，用键盘控制turtlebot运动
$ roslaunch turtlebot3_slam turtlebot3_slam.launch slam_methods:=karto
#这里也可以选择其他的slam方法,例如gmapping
通过手动控制机器人turtlebot3完成建图操作后，需要将完整的地图保存下来，以便导航时调用。
$ mkdir -p ~/maps/mymap
$ rosrun map_server map_saver -f ~/maps/mymap/mymap
具体结果如下：


3、turtlebot3导航实验
1）安装navigation相关的ros包
$ sudo apt-get install ros-noetic-navigation
2）利用刚才构建的地图进行turtlebot3的自主导航仿真实验
开启终端，运行：
$ export TURTLEBOT3_MODLE=burger
$ roslaunch turtlebot3_gazebo turtlebot3_world.launch
在新的一个终端，运行：
$ export TURTLEBOT3_MODLE=burger
$ roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=/home/gfeng/maps/mymap/mymap.yaml

按下“2D Pose Estimate”调整初始位置到正确的地方，使得上图发亮的边框与实际的位置重合，具体如下：

此时按“2D Nav Goal”选择导航的终点位置，如下图：

最终turtlebot3会自动导航到目标位置，并调整最终方向。













实验七  Turtlebot3深度强化学习DQN开发环境构建
一、实验目的和要求
1、了解仿真环境下机器人turtlebot3深度学习DQN开发环境的关键；
二、实验内容	
1、下载turtlebot3相关程序源码；
2、运行深度强化学习；
3、分析DQN代码；
4、修改DQN代码。
5、按实验步骤进行，并拍摄视频记录实验过程；
6、完成实验报告并对实验过程进行总结。
三、实验项目
DQN应用开发实践。
四、实验器材和环境
1、硬件环境：PC机；
2、软件环境：windows10、VMware、Ubuntu 16.04；
五、实验步骤
1、anaconda2安装
ROS 1中使用的是 python2.7，所以在地址：https://www.anaconda.com/download/#linux 中下载 python2.7版本的Anaconda2 ，安装过程如下：
$ chmod +x Anaconda2-5.2.0-Linux-x86_64.sh 
$ bash Anaconda2-5.2.0-Linux-x86_64.sh
安装完成后，执行下面指令, 如果可以看到 Python 2.7.xx :: Anaconda, Inc..，说明安装成功。
$ source ~/.bashrc
$ python -V
接下来配置环境变量 PYTHONPATH，编辑 ~/.bashrc ，修改或者添加内容如下：
#export PYTHONPATH="/usr/lib/python2.7/dist-packages:$PYTHONPATH" 
export PYTHONPATH="/home/用户目录/anaconda2/lib/python2.7/dist-packages:$PYTHONPATH
2、Tensorflow、Keras等安装
由于用到的机器学习算法是DQN（Deep Q-Learning），基于Tensorflow与Keras开发，为了避免包冲突，在Anaconda中构建虚拟环境（取名为tensorflow）:
$ conda update -n base conda
$ conda create -n tensorflow pip python=2.7
进入或者离开虚拟环境的指令为：
# To activate this environment, use:
$ source activate tensorflow
# To deactivate an active environment, use:
$ source deactivate
 
在虚拟环境（envs）中安装Tensorflow、Keras以及ROS依赖包：
$ source activate tensorflow 
#为了使ROS 与 Anaconda 一起工作，必须安装在Anaconda中安装ROS 依赖包
$ pip install -U rosinstall msgpack empy defusedxml netifaces 
$ pip install --ignore-installed --upgrade https://download.tensorflow.google.cn/linux/gpu/ tensorflow_gpu-1.8.0-cp27-none-linux_x86_64.whl
$ pip install keras==2.1.6
$ source deactivate
这里安装的是gpu版本的tensorflow,前提是有gpu显卡并安装配置好了相关驱动， 如果要安装cpu版本的tensorflow，指令中地址应为：https://download.tensorflow.google.cn/linux/cpu/tensorflow-1.8.0-cp27-none-linux_x86_64.whl。

3、下载并编译源码
先使用github中开源的机器学习的源码进行学习，下载编译过程如下：
$ cd ~/catkin_ws/src/
$ git clone https://github.com/ROBOTIS-GIT/turtlebot3_msgs.git
$ git clone https://github.com/ROBOTIS-GIT/turtlebot3.git
$ git clone https://github.com/ROBOTIS-GIT/turtlebot3_simulations
$ git clone https://github.com/ROBOTIS-GIT/turtlebot3_machine_learning
$ cd ~/catkin_ws && catkin_make
编译成功以后执行下面的脚本,也可以将其添加到~/.bashrc中：
$ source /home/wsc/catkin_ws/devel/setup.bash

4、设置参数并运行范例
设置参数：
打开源码文件 turtlebot3/turtlebot3_description/urdf/turtlebot3_burger.gazebo.xacro，修改一下两处：
<xacro:arg name="laser_visual" default="false"/>   # 如果想看到激光扫描线,设置成 `true`
<scan>
  <horizontal>
    <samples>360</samples>            # 修改成24
    <resolution>1</resolution>
    <min_angle>0.0</min_angle>
    <max_angle>6.28319</max_angle>
  </horizontal>
</scan>
打开一个终端，启动turtlebot3 gazebo环境等节点：
$ export SVGA_VGPU10=0
$ export TURTLEBOT3_MODEL=burger
$ roslaunch turtlebot3_gazebo turtlebot3_stage_1.launch
打开另外一个终端，启动DQN算法等节点：
$ source activate tensorflow
$ roslaunch turtlebot3_dqn turtlebot3_dqn_stage_1.launch
打开第三个终端，启动数据图形显示节点：
$ pip install pyqtgraph
$ roslaunch turtlebot3_dqn result_graph.launch
注：源码范例中提供了turtlebot3_stage_1～turtlebot3_stage_4共4个环境，分别是无障碍、静态障碍、动态障碍、混合障碍环境。
如果找不到PyQt5，在conda建立DRL环境下安装
$ sudo apt-get install python-pyqt5-dbg
并设置
export PYTHONPATH="/home/gfeng/anaconda2/lib/python2.7/site-packages/:$PYTHONPATH"
$ pip install -U rosdep rosinstall_generator wstool rosinstall six vcstools
$ pip install msgpack grin











1解决Vmware下虚拟机下打开gazebo报错：VMware: vmw_ioctl_command error Invalid argument
引言
在使用虚拟机Vmware打开gazebo仿真环境的时候，刚打开就闪退，并报错以下错误：VMware: vmw_ioctl_command error Invalid argument,大概意思是虚拟机参数不合法，这应该是虚拟机的bug，毕竟使用虚拟机和真实的物理机上是有差别的，不过这个bug有网友已经解决了，大致有以下两种解决方案；

解决方案一：关闭虚拟机的3D图形加速
如下图，在编辑虚拟机选项中关闭3D图形加速，可以解决此问题；


这种方法简单粗暴，但是没有了3D图形加速功能意味着我们的虚拟机不能使用主机的GPU，而我们做仿真是需要GPU做图形加速的，所以这种方案长期来看不可取，下面有更好的解决方案。

解决方案二：设置环境变量
上面我们也讲到了，报错的原因大概是不合法的变量导致的，应该是启动gazebo的时候加载系统的某个环境变量的时候虚拟机无法完美识别导致的，所以知道了原因后，追根溯源，我们找到了以下修复方案，加入环境变量：

export SVGA_VGPU10=0
1
在shell中运行以上语句，但是这种方法有个坏处就是每次打开新的shell这个环境变量就失效了，所以我们需要将其加入到用户的环境变量，这样每次启动就可以自动运行此码；

echo "export SVGA_VGPU10=0" >> ~/.bashrc
1
加入以上代码即可完美解决虚拟机的这个问题，具体的原因我也不清楚，以上解决方案也是在网上搜集的网友的方法，具体为啥也不从而知，不过解决了问题就可以了，感谢这些网友；

结语
以上就是怎么解决的虚拟机的这个bug，不过建议是在物理机上运行linux，这样可以少去很多bug烦恼，不过实在是没有第二台电脑的话，就只能在虚拟机玩玩了，或者是租一个服务器用docker玩玩，不过也挺花钱的，哈哈！

ROS开发笔记（9）——ROS 深度强化学习应用之KERAS版本DQN代码分析
在ROS开发笔记（8）中构建了ROS中DQN算法的开发环境，在此基础上，对算法代码进行了分析，并做了简单的修改：
修改1  ： 改变了保存模型参数在循环中的位置，原来是每个10整数倍数回合里面每一步都修改（相当于修改episode_step次），改成了每个10整数倍数回合修改一次
# if e % 10 == 0:
#             agent.model.save(agent.dirPath + str(e) + '.h5')
#             with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
#                 param_keys = ['epsilon']
#                 param_values = [agent.epsilon]
#                 param_dictionary = dict(zip(param_keys, param_values))
#                 json.dump(param_dictionary, outfile)

 修改2 ：改变了agent.updateTargetModel()的位置，原来是每次done都修改，改成了每经过target_up步后修改
# if global_step % agent.target_update == 0:
#     agent.updateTargetModel()
 #     rospy.loginfo("UPDATE TARGET NETWORK")
结果如下：

#!/usr/bin/env python
#-*- coding:utf-8   -*-
 
#################################################################################
# Copyright 2018 ROBOTIS CO., LTD.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#################################################################################
 
# 原作者
# Authors: Gilbert #
 
 
 
import rospy
import os
import json
import numpy as np
import random
import time
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
 
from keras.models import Sequential, load_model
from keras.optimizers import RMSprop
from keras.layers import Dense, Dropout, Activation
 
# 导入 Env
from src.turtlebot3_dqn.environment_stage_1 import Env
#最大回合数
EPISODES = 3000
 
#强化学习网络
class ReinforceAgent():
    #初始化函数
    def __init__(self, state_size, action_size):
        # 创建 result 话题
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        # 获取当前文件完整路径
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        # 基于当前路径生成模型保存路径前缀
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/save_model/stage_1_')
        # 初始化 result 话题
        self.result = Float32MultiArray()
 
        #wsc self.load_model = False
        #wsc self.load_episode = 0
 
        # 导入前期训练的模型
        self.load_model =True
        self.load_episode = 150
        # self.load_model =False
        # self.load_episode = 0
        
        #状态数 
        self.state_size = state_size
        #动作数
        self.action_size = action_size
        # 单个回合最大步数
        self.episode_step = 6000
 
        # 每2000次更新一次target网络参数
        self.target_update = 2000
 
        # 折扣因子 计算reward时用 当下反馈最重要 时间越久的影响越小
        self.discount_factor = 0.99
 
        # 学习率learning_rate  学习率决定了参数移动到最优值的速度快慢。
        # 如果学习率过大，很可能会越过最优值；反而如果学习率过小，优化的效率可能过低，长时间算法无法收敛。
        self.learning_rate = 0.00025
 
        # 初始ϵ——epsilon
        # 探索与利用原则
        # 探索强调发掘环境中的更多信息，并不局限在已知的信息中；
        # 利用强调从已知的信息中最大化奖励；
        # greedy策略只注重了后者，没有涉及前者；
        # ϵ-greedy策略兼具了探索与利用，它以ϵ的概率从所有的action中随机抽取一个，以1−ϵ的概率抽取能获得最大化奖励的action。
        self.epsilon = 1.0
        
        #随着模型的训练，已知的信息越来越可靠，epsilon应该逐步衰减
        self.epsilon_decay = 0.99
 
        #最小的epsilon_min，低于此值后不在利用epsilon_decay衰减
        self.epsilon_min = 0.05
        
        #batch_size 批处理大小
        # 合理范围内，增大 Batch_Size
        # 内存利用率提高了，大矩阵乘法的并行化效率提高
        # 跑完一次epoch（全数据集）所需要的迭代次数减小，对于相同数据量的处理速度进一步加快
        # 在一定范围内，一般来说batch size越大，其确定的下降方向越准，引起的训练震荡越小
 
        # 盲目增大batch size 有什么坏处
        # 内存利用率提高了，但是内存容量可能撑不住了
        # 跑完一次epoch（全数据集）所需要的迭代次数减少，但是想要达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢
        # batch size 大到一定的程度，其确定的下降方向已经基本不再变化
        self.batch_size = 64
 
        # 用于 experience replay 的 agent.memory
        # DQN的经验回放池(agent.memory)大于train_start才开始训练网络(agent.trainModel)        
        self.train_start = 64
 
        # 用队列存储experience replay 数据，并设置队列最大长度
        self.memory = deque(maxlen=1000000)
 
        # 网络模型构建
        self.model = self.buildModel()
 
        #target网络构建
        self.target_model = self.buildModel()
 
        self.updateTargetModel()
 
        # 训练可以加载之前保存的模型参数进行 
        if self.load_model:
            self.model.set_weights(load_model(self.dirPath+str(self.load_episode)+".h5").get_weights())
 
            with open(self.dirPath+str(self.load_episode)+'.json') as outfile:
                param = json.load(outfile)
                self.epsilon = param.get('epsilon')
                #wsc self.epsilon = 0.5
    
    # 网络模型构建
    def buildModel(self):
 
        # Sequential序列模型是一个线性的层次堆栈
        model = Sequential()
 
        # 设置dropout，防止过拟合
        dropout = 0.2
 
        # 添加一层全连接层，输入大小为input_shape=(self.state_size,)，输出大小为64，**函数为relu，权值初始化方法为lecun_uniform
        model.add(Dense(64, input_shape=(self.state_size,), activation='relu', kernel_initializer='lecun_uniform'))
 
        # 添加一层全连接层，输出大小为64，**函数为relu，权值初始化方法为lecun_uniform
        model.add(Dense(64, activation='relu', kernel_initializer='lecun_uniform'))
        # 添加dropout层
        model.add(Dropout(dropout))
 
        # 添加一层全连接层，输出大小为action_size，权值初始化方法为lecun_uniform
        model.add(Dense(self.action_size, kernel_initializer='lecun_uniform'))
       
        # 添加一层linear**层
        model.add(Activation('linear'))
 
        # 优化算法RMSprop是AdaGrad算法的改进。鉴于神经网络都是非凸条件下的，RMSProp在非凸条件下结果更好，改变梯度累积为指数衰减的移动平均以丢弃遥远的过去历史。
        # 经验上，RMSProp被证明有效且实用的深度学习网络优化算法。rho=0.9为衰减系数，epsilon=1e-06为一个小常数，保证被小数除的稳定性
        model.compile(loss='mse', optimizer=RMSprop(lr=self.learning_rate, rho=0.9, epsilon=1e-06))
 
        # model.summary()：打印出模型概况
        model.summary()
 
        return model
    
    # 计算Q值，用到reward（当前env回馈），done，以及有taget_net网络计算得到的next_target
    def getQvalue(self, reward, next_target, done):
        if done:
            return reward
        else:
            return reward + self.discount_factor * np.amax(next_target)
   
    # eval_net用于预测 q_eval 
    # target_net 用于预测 q_target 值
    # 将eval net权重赋给target net 
    def updateTargetModel(self):
        self.target_model.set_weights(self.model.get_weights())
    
    #基于ϵ——epsilon策略选择动作
    def getAction(self, state):
        if np.random.rand() <= self.epsilon:
            self.q_value = np.zeros(self.action_size)
            return random.randrange(self.action_size)
        else:
            q_value = self.model.predict(state.reshape(1, len(state)))
            self.q_value = q_value
            return np.argmax(q_value[0])
    
    #将经验数据存入经验池 当前状态state，基于当前状态选择的动作action，执行动作获得的回报reward，执行动作后环境变成的next_state，以及done
    def appendMemory(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
 
    # 训练网络模型
    def trainModel(self, target=False):
        mini_batch = random.sample(self.memory, self.batch_size)
        X_batch = np.empty((0, self.state_size), dtype=np.float64)
        Y_batch = np.empty((0, self.action_size), dtype=np.float64)
 
        for i in range(self.batch_size):
            states = mini_batch[i][0]
            actions = mini_batch[i][1]
            rewards = mini_batch[i][2]
            next_states = mini_batch[i][3]
            dones = mini_batch[i][4]
            # 计算q_value
            q_value = self.model.predict(states.reshape(1, len(states)))
            self.q_value = q_value
 
            #计算next_target
            if target:
                next_target = self.target_model.predict(next_states.reshape(1, len(next_states)))
 
            else:
                next_target = self.model.predict(next_states.reshape(1, len(next_states)))
 
            # 计算 next_q_value
            next_q_value = self.getQvalue(rewards, next_target, dones)
 
            X_batch = np.append(X_batch, np.array([states.copy()]), axis=0)
            Y_sample = q_value.copy()
 
            Y_sample[0][actions] = next_q_value
            Y_batch = np.append(Y_batch, np.array([Y_sample[0]]), axis=0)
 
            if dones:
                X_batch = np.append(X_batch, np.array([next_states.copy()]), axis=0)
                Y_batch = np.append(Y_batch, np.array([[rewards] * self.action_size]), axis=0)
 
        self.model.fit(X_batch, Y_batch, batch_size=self.batch_size, epochs=1, verbose=0)
 
if __name__ == '__main__':
    rospy.init_node('turtlebot3_dqn_stage_1')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()
 
    state_size = 26
    action_size = 5
 
    env = Env(action_size)
 
    agent = ReinforceAgent(state_size, action_size)
    scores, episodes = [], []
    global_step = 0
    start_time = time.time()
 
 
    # 循环EPISODES个回合 
    for e in range(agent.load_episode + 1, EPISODES):
        done = False
        state = env.reset()
        score = 0
 
        # 每10个回合保存一次网络模型参数
        if e % 10 == 0:
            agent.model.save(agent.dirPath + str(e) + '.h5')
            with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
                param_keys = ['epsilon']
                param_values = [agent.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                json.dump(param_dictionary, outfile)
        
        # 每个回合循环episode_step步
        for t in range(agent.episode_step):
            # 选择动作
            action = agent.getAction(state)
            # Env动作一步，返回next_state, reward, done
            next_state, reward, done = env.step(action)
            # 存经验值
            agent.appendMemory(state, action, reward, next_state, done)
 
            # agent.memory至少要收集agent.train_start（64）个才能开始训练
            # global_step 没有达到 agent.target_update之前要用到target网络的地方由eval代替
            if len(agent.memory) >= agent.train_start:
                if global_step <= agent.target_update:
                    agent.trainModel()
                else:
                    agent.trainModel(True)
            # 将回报值累加成score
            score += reward
 
            state = next_state
            # 发布 get_action 话题
            get_action.data = [action, score, reward]
            pub_get_action.publish(get_action)
 
            
            # 超过500步时设定为超时，回合结束
            if t >= 500:
                rospy.loginfo("Time out!!")
                done = True
 
            if done:
                # 发布result话题
                result.data = [score, np.max(agent.q_value)]
                pub_result.publish(result)
                scores.append(score)
                episodes.append(e)
                # 计算运行时间
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)
 
                rospy.loginfo('Ep: %d score: %.2f memory: %d epsilon: %.2f time: %d:%02d:%02d',
                              e, score, len(agent.memory), agent.epsilon, h, m, s)
                break
 
            global_step += 1
            # 每经过agent.target_update，更新target网络参数
            if global_step % agent.target_update == 0:
                agent.updateTargetModel()
                rospy.loginfo("UPDATE TARGET NETWORK")
        
        # 更新衰减epsilon值，直到低于等于agent.epsilon_min
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay


2ROS开发笔记（10）——ROS 深度强化学习dqn应用之tensorflow版本(double dqn/dueling dqn/prioritized replay dqn)
ROS开发笔记（10）——ROS 深度强化学习dqn应用之tensorflow版本(double dqn/dueling dqn/prioritized replay dqn)
在ROS开发笔记（9）中keras版本DQN算法基础上，参考莫烦强化学习的视频教程与代码，编写了应用在ROS中的tensorflow版本DQN算法，本部分代码包含了原始dqn、double dqn、dueling dqn、prioritized replay dqn四种版本的实现：

1、原始DQN

DQN利用神经网络计算Q值，拥有 经验回放（Experience replay） 和 周期性冻结 target 网络参数（Fixed Q-targets）的特性，Experience replay可以充分利用历史数据，Fixed Q-targets可以切断相关性，网络结构图如下：

2、double dqn

double dqn 是用来解决过估计的，其与原始dqn的结构基本相同，只是计算q_target的方法不同，具体参见代码。

3、prioritized replay dqn

前面两种dqn在经验回放时是随机采样经验池中的数据，prioritized replay dqn是按照 Memory 中的样本优先级来抽取，以便能更有效地找到需要学习的样本。

这里用（ Q现实 - Q估计 ）来规定优先学习的程度. 如果 其差 越大, 就代表预测精度还有很多上升空间, 那么这个样本就越需要被学习, 也就是优先级 p 越高。

4、dueling dqn

dueling dqn算法主要改变是将Q值分为两部分，一部分是状态本身带来的Q值，下图中用value表示，一部分是这个状态下不同的动作产生的不同的Q值，下图中用advantage表示，然后将两种相加得到最终的Q值。

dueling dqn与prioritized replay dqn在一些特定的场合会体现出相对优越性，在我们这个应用场景中效果差不多，这里主要是实现了这几种算法结构，具体的代码及注释如下：
#!/usr/bin/env python
#-*- coding:utf-8   -*-
 
import rospy
import random
import time
import tensorflow as tf
import numpy as np
import json
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray
 
# 导入 Env
from src.turtlebot3_dqn.environment_stage_1 import Env
 
# 采用莫烦强化学习教程中的实现
class SumTree(object):
    data_pointer = 0
    def __init__(self,capacity):
        # capacity 为回放经验的条数
        self.capacity=capacity
        # 初始化tree
        self.tree=np.zeros(2*capacity-1)
        # 初始化回放经验数据
        self.data=np.zeros(capacity,dtype=object)
    
    def add(self,p,data):
        # tree_idx 为所加data在树中的索引号
        self.tree_idx=self.data_pointer+self.capacity-1
        self.data[self.data_pointer]=data
        # 更新树
        self.update(self.tree_idx,p)
        self.data_pointer+=1
        if self.data_pointer>=self.capacity:
            self.data_pointer=0
 
    def update(self,tree_idx,p):
        change=p-self.tree[tree_idx]
        self.tree[tree_idx]=p
        # 更新tree的p
        while tree_idx!=0:
            # 更新父节点的p值
            tree_idx = (tree_idx - 1) // 2
            self.tree[tree_idx] += change
 
    def get_leaf(self,v):
        parent_idx = 0
        # 根据V寻找对应的叶子节点
        while True:    
            cl_idx = 2 * parent_idx + 1        
            cr_idx = cl_idx + 1
            # 判断是否达到树的底部
            if cl_idx >= len(self.tree):       
                leaf_idx = parent_idx
                break
            else:       
                if v <= self.tree[cl_idx]:
                    parent_idx = cl_idx
                else:
                    v -= self.tree[cl_idx]
                    parent_idx = cr_idx
 
        data_idx = leaf_idx - self.capacity + 1
        # 输出叶子节点序号，p值，以及对应的数据
        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]
    
    @property
    def total_p(self):
        return self.tree[0] 
 
 
# 采用莫烦强化学习教程中的实现
class Memory(object):
    # 实际保存数据的条数
    saved_size=0
    epsilon = 0.01  # 避免 0 priority
    alpha = 0.6  # [0~1] 将 importance of TD error转化为 priority
    beta = 0.4  # importance-sampling 从这个初始值增加到1
    beta_increment_per_sampling = 0.001 
    abs_err_upper = 50.      
    def __init__(self,capacity):
        self.tree=SumTree(capacity)
 
    def store(self,transition):
        # 将新加入的transition 优先级p设为最高
        max_p=np.max(self.tree.tree[-self.tree.capacity:])
        if max_p==0:
            max_p=self.abs_err_upper
        self.tree.add(max_p,transition)
        if self.saved_size<self.tree.capacity:
            self.saved_size+=1
    
    def sample(self,n):
        # 初始化 b_idx, b_memory, ISWeights
        b_idx,  ISWeights = np.empty((n,), dtype=np.int32),  np.empty((n, 1))
        b_memory=[]
        self.beta=np.min([1.,self.beta+self.beta_increment_per_sampling])
        min_prob = np.min(self.tree.tree[self.tree.capacity-1:self.tree.capacity-1+self.saved_size]) / self.tree.total_p
        # print(self.tree.tree[self.tree.capacity-1:self.tree.capacity-1+self.saved_size]) 
 
        # 将total_p分为n份，每一份为pri_seg
        pri_seg = self.tree.total_p / n  
        for i in range(n):
            a,b= pri_seg*i, pri_seg*(i+1)
            v=random.uniform(a,b)
            idx, p, data=self.tree.get_leaf(v)
            prob=p/self.tree.total_p
            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)
            b_idx[i]= idx
            b_memory.append(data)
        return b_idx, b_memory, ISWeights
 
    def batch_update(self,tree_idx,abs_errors):
        abs_errors += self.epsilon  # 避免0
        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)
        ps = np.power(clipped_errors, self.alpha)
        for ti, p in zip(tree_idx, ps):
            self.tree.update(ti, p)
 
class DqnAgent():
    def __init__(self,
                 state_size,
                 action_size,
                 learning_rate = 0.00025,
                 drop_out=0.2,
                 discount_factor = 0.99,
                 epsilon = 1,
                 epsilon_decay=0.99,
                 epsilon_min=0.05,
                 episodes = 3000,
                 episode_step=6000,
                 target_update=2000,
                 memory_size = 1000000,
                 batch_size = 64,
                 output_graph = False,
                 summaries_dir="logs/",
                 sess=None,
                 double_dqn = True, 
                 prioritized=False,
                 dueling=True
                 ):
        self.dueling=dueling
        self.sess = sess
        self.prioritized=prioritized
        self.double_dqn = double_dqn
        # 创建 result 话题
        self.pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
        # 初始化 result 话题
        self.result = Float32MultiArray()
 
        # 训练时的步数
        self.global_step = 0
 
        # 是否输出可视化图形,tensorboard相关
        self.output_graph=output_graph
 
 
        # 获取当前文件完整路径
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        # 基于当前路径生成模型保存路径前缀
        self.dirPath = self.dirPath+'/save_model/data_'
 
        # 导入前期训练的模型
        # self.load_model = True
        # self.load_episode = 150
        self.load_model = False
        self.load_episode = 0
 
        # 状态数
        self.state_size = state_size
        # 动作数
        self.action_size = action_size
 
        # 最大回合数
        self.episodes = episodes
 
        # 单个回合最大步数
        self.episode_step = episode_step
 
        # 每2000次更新一次target网络参数
        self.target_update=target_update
 
        # 折扣因子 计算reward时用 当下反馈最重要 时间越久的影响越小
        self.discount_factor=discount_factor
 
        # 学习率learning_rate  学习率决定了参数移动到最优值的速度快慢。
        # 如果学习率过大，很可能会越过最优值；反而如果学习率过小，优化的效率可能过低，长时间算法无法收敛。
        self.learning_rate=learning_rate
 
        # dropout层的rate
        self.drop_out=drop_out
 
        # 初始ϵ——epsilon
        # 探索与利用原则
        # 探索强调发掘环境中的更多信息，并不局限在已知的信息中；
        # 利用强调从已知的信息中最大化奖励；
        # greedy策略只注重了后者，没有涉及前者；
        # ϵ-greedy策略兼具了探索与利用，它以ϵ的概率从所有的action中随机抽取一个，以1−ϵ的概率抽取能获得最大化奖励的action。
        self.epsilon=epsilon
 
        # 随着模型的训练，已知的信息越来越可靠，epsilon应该逐步衰减
        self.epsilon_decay=epsilon_decay
 
        # 最小的epsilon_min，低于此值后不在利用epsilon_decay衰减
        self.epsilon_min=epsilon_min
 
        # batch_size 批处理大小
        # 合理范围内，增大 Batch_Size
        # 内存利用率提高了，大矩阵乘法的并行化效率提高
        # 跑完一次epoch（全数据集）所需要的迭代次数减小，对于相同数据量的处理速度进一步加快
        # 在一定范围内，一般来说batch size越大，其确定的下降方向越准，引起的训练震荡越小
 
        # 盲目增大batch size 有什么坏处
        # 内存利用率提高了，但是内存容量可能撑不住了
        # 跑完一次epoch（全数据集）所需要的迭代次数减少，但是想要达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢
        # batch size 大到一定的程度，其确定的下降方向已经基本不再变化
        self.batch_size = batch_size
 
        # 用于 experience replay 的 agent.memory
        # DQN的经验回放池(agent.memory)大于train_start才开始训练网络(agent.trainModel)
        self.train_start = self.batch_size
 
        # 用队列存储experience replay 数据，并设置队列最大长度
        if self.prioritized:
            self.memory=Memory(capacity=memory_size)
        else:
            self.memory=deque(maxlen=memory_size)
 
        # tensorboard保存路径
        self.summaries_dir = summaries_dir
 
        # 创建网络模型 [target_net, evaluate_net]
        self._buildModel()
 
        # 利用eval_net网络参数给target_net网络赋值
        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')
        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')
 
        with tf.variable_scope('soft_replacement'):
            self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]
        
        if sess is None:
            self.sess = tf.Session()           
        else:
            self.sess = sess
 
        # tensorboard 可视化相关
        if self.output_graph:
            # 终端中执行 tensorboard --logdir=logs 命令，浏览器中输入 http://127.0.0.1:6006/ 可在tensorboard查看模型与数据
            # merged 也是一个操作，在训练时执行此操作
            self.merged = tf.summary.merge_all()
            # 创建 summary_writer
            self.summary_writer = tf.summary.FileWriter("logs/", self.sess.graph)
            print(self.summary_writer)
 
        # 2小时最多存5次
        self.saver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=2)
 
        self.sess.run(tf.global_variables_initializer())
 
        # 训练可以加载之前保存的模型参数进行
        if self.load_model:
            self.saver.restore(self.sess, tf.train.latest_checkpoint('./checkpoint_dir/SaveModelDoubleDqnTf'))
 
            with open(self.dirPath + str(self.load_episode) + '.json') as outfile:
                param = json.load(outfile)
                self.epsilon = param.get('epsilon')
                # wsc self.epsilon = 0.5
                self.epsilon = 0.5
 
 
    def _buildModel(self):
 
        # ------------------ all inputs -----------------------------
        self.s = tf.placeholder(tf.float32, shape=[None, self.state_size], name='s')
        self.s_ = tf.placeholder(tf.float32, shape=[None, self.state_size], name='s_')
        self.q_target = tf.placeholder(tf.float32, [None, self.action_size], name='q_target')  
 
        if self.prioritized:
            self.ISWeights = tf.placeholder(tf.float32, [None, 1], name='IS_weights')
 
        # self.a = tf.placeholder(tf.int32, shape=[None, ], name='a')
 
        # net_config
        w_initializer = tf.random_normal_initializer(0., 0.3) 
        b_initializer = tf.constant_initializer(0.1)
 
        # ------------------ built evaluate_net ---------------------
        with tf.variable_scope('eval_net'):
            e1 = tf.layers.dense(self.s, 64, activation=tf.nn.relu, kernel_initializer=w_initializer,
                                 bias_initializer=b_initializer, name='e1')
            e2 = tf.layers.dense(e1, 64, activation=tf.nn.relu, kernel_initializer=w_initializer,
                                 bias_initializer=b_initializer, name='e2')
            e3 = tf.layers.dropout(e2, rate=self.drop_out, name='eval_drop_out')
            # self.q_eval = tf.layers.dense(e3, self.action_size,activation=tf.nn.softmax,kernel_initializer=w_initializer,
            #                               bias_initializer=b_initializer, name='q')
 
            if self.dueling:
                with tf.variable_scope('value'):
                    self.base_value = tf.layers.dense(e3, 1,kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='value')
                with tf.variable_scope('advantage'):
                    self.advantage_value = tf.layers.dense(e3, self.action_size,kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='advantage')
                with tf.variable_scope('Q'):
                    self.q_eval=self.base_value+(self.advantage_value-tf.reduce_mean(self.advantage_value, axis=1, keep_dims=True))
            else:
                self.q_eval = tf.layers.dense(e3, self.action_size,kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='q')        
        
        with tf.variable_scope('loss'):
            if self.prioritized:
                self.abs_errors = tf.reduce_sum(tf.abs(self.q_target - self.q_eval), axis=1)    # for updating Sumtree
                self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval,name='TD_error'))
            else:
                self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval,name='TD_error'))
        
        # 添加scalar类型summary
        tf.summary.scalar('loss', self.loss)
 
        with tf.variable_scope('train'):
            self._train_op = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)
 
 
        # ------------------ built target_net -----------------------
        with tf.variable_scope('target_net'):
            t1 = tf.layers.dense(self.s_, 64, activation=tf.nn.relu, kernel_initializer=w_initializer,
                                 bias_initializer=b_initializer, name='t1')
            t2 = tf.layers.dense(t1, 64, activation=tf.nn.relu, kernel_initializer=w_initializer,
                                 bias_initializer=b_initializer, name='t2')
            t3 = tf.layers.dropout(t2, rate=self.drop_out, name='target_drop_out')
 
            if self.dueling:
                with tf.variable_scope('t_value'):
                    self.t_base_value = tf.layers.dense(t3, 1,kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='t_value')
                with tf.variable_scope('t_advantage'):
                    self.t_advantage_value = tf.layers.dense(t3, self.action_size,kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='t_advantage')
                with tf.variable_scope('t_Q'):
                    self.q_next=self.t_base_value+(self.t_advantage_value-tf.reduce_mean(self.t_advantage_value, axis=1, keep_dims=True))
 
            else:
                # self.q_next = tf.layers.dense(t3, self.action_size, activation=tf.nn.softmax,
                #                               kernel_initializer=w_initializer,
                #                               bias_initializer=b_initializer, name='t3')
                self.q_next = tf.layers.dense(t3, self.action_size, 
                                            kernel_initializer=w_initializer,
                                            bias_initializer=b_initializer, name='t3')
    # 训练网络模型
    def trainModel(self, target=False):
        # 每经过target_update，更新target网络参数
        if self.global_step % self.target_update == 0:
            self.updateTargetModel()
            rospy.loginfo("UPDATE TARGET NETWORK")
            print("UPDATE TARGET NETWORK")           
 
        if self.prioritized:
            tree_idx, mini_batch, ISWeights = self.memory.sample(self.batch_size)
        else:
            mini_batch = random.sample(self.memory, self.batch_size)
 
 
        state_batch = np.empty((0, self.state_size), dtype=np.float64)
        action_batch = np.empty((0, ), dtype=np.float64)
        reward_batch = np.empty((0, ), dtype=np.float64)
        state_next_batch = np.empty((0, self.state_size), dtype=np.float64)
        q_target_batch=np.empty((0, self.action_size), dtype=np.float64)
 
        for i in range(self.batch_size):
            # states: [-0.25  0.  ]
            # states.reshape(1, len(states)):[[-0.5  0. ]] ,一行数据
            # print(mini_batch[i]) (array([-0.5,  0. ]), 2, 0, array([-0.25,  0.  ]), False)
            state = mini_batch[i][0]
            action = mini_batch[i][1]
            reward = mini_batch[i][2]
            next_state = mini_batch[i][3]
            # done = mini_batch[i][4]
 
            # state=state.reshape(1,len(state))
            # next_state=next_state.reshape(1,len(next_state))
 
            # 将next_state 分别送入 target_net 与 eval_net 分别获得 q_next, q_eval_next
            q_next, q_eval_next = self.sess.run(
                [self.q_next, self.q_eval],
                feed_dict={self.s_: [next_state], self.s: [next_state]})  
 
            # 将 state 送入  eval_net  获得  q_eval
            q_eval = self.sess.run(self.q_eval, {self.s: [state]})
 
            q_target = q_eval.copy()
 
            if self.double_dqn:
                # double DQN
                # 选择 q_eval_next 向量中最大值对应的动作序号
                max_act_next_state = np.argmax(q_eval_next, axis=1) 
                # Double DQN, 根据动作序号选择 q_next 值
 
                selected_q_next = q_next[0,max_act_next_state]  
            else:
                 # DQN
                selected_q_next = np.max(q_next, axis=1)   
            
            q_target[0,action] = reward + self.discount_factor * selected_q_next
 
            state_batch = np.append(state_batch, np.array([state.copy()]), axis=0)
 
            q_target_batch=np.append(q_target_batch, np.array(q_target.copy()), axis=0)
                       
            action_batch = np.append(action_batch, np.array([action]), axis=0)
            reward_batch = np.append(reward_batch, np.array([reward]), axis=0)
            state_next_batch = np.append(state_next_batch, np.array([next_state.copy()]), axis=0)       
        
        # tensorboard 可视化相关
        if self.output_graph:
            if self.prioritized:
                summary,_, abs_errors, self.cost ,self.q_value= self.sess.run([self.merged, self._train_op, self.abs_errors, self.loss,self.q_eval],
                                            feed_dict={self.s: state_batch,
                                                        self.q_target: q_target_batch,
                                                        self.ISWeights: ISWeights})
                self.memory.batch_update(tree_idx, abs_errors)    
            else:
                # 这里运行了self.merged操作
                summary, _,self.q_value = self.sess.run([self.merged, self._train_op,self.q_eval],
                                        feed_dict={
                                            self.s: state_batch,
                                            self.q_target: q_target_batch
                                        })
                # 保存 summary
                self.summary_writer.add_summary(summary, self.global_step)
        else:
             if self.prioritized:
                _, abs_errors, self.cost,self.q_value = self.sess.run([self._train_op, self.abs_errors, self.loss,self.q_eval],
                                            feed_dict={self.s: state_batch,
                                                        self.q_target: q_target_batch,
                                                        self.ISWeights: ISWeights})
                self.memory.batch_update(tree_idx, abs_errors)  
             else:
                 _ ,self.q_value= self.sess.run([self._train_op,self.q_eval],
                                        feed_dict={
                                            self.s: state_batch,
                                            self.q_target: q_target_batch
                                        })
 
    def updateTargetModel(self):
        self.sess.run(self.target_replace_op)
        print('\ntarget_params_replaced\n')
 
    def appendMemory(self, state, action, reward, next_state, done):
        if self.prioritized:    
            transition = (state, action, reward, next_state)
            self.memory.store(transition)    
        else:       
            self.memory.append((state, action, reward, next_state, done))
 
 
    # 基于ϵ——epsilon策略选择动作
    def chooseAction(self, state):
        if np.random.rand() <= self.epsilon:
            action = random.randrange(self.action_size)
            return action
 
        else:
            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: [state]})
            self.q_value = actions_value
            action = np.argmax(actions_value)
            return action
 
 
if __name__ == '__main__':
 
    rospy.init_node('turtlebot3_dqn_stage_1_tensorflow')
    pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
    pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
    result = Float32MultiArray()
    get_action = Float32MultiArray()
 
    state_size = 26
    action_size = 5
 
    env = Env(action_size)
 
    agent = DqnAgent(state_size, action_size,output_graph=True)
    scores, episodes = [], []
    agent.global_step = 0
    start_time = time.time()
 
    # 循环EPISODES个回合
    for e in range(agent.load_episode + 1, agent.episodes):
        done = False
        state = env.reset()
        score = 0
 
        # 每10个回合保存一次网络模型参数
        if e % 10 == 0:
            # 保存参数,不保存graph结构
            agent.saver.save(agent.sess, "./checkpoint_dir/SaveModelDoubleDqnTf", global_step=e, write_meta_graph=True)
            with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
                param_keys = ['epsilon']
                param_values = [agent.epsilon]
                param_dictionary = dict(zip(param_keys, param_values))
                json.dump(param_dictionary, outfile)
                print('epsilon saver')
 
        # 每个回合循环episode_step步
        for t in range(agent.episode_step):
 
            # 选择动作
            action = agent.chooseAction(state)
            # Env动作一步，返回next_state, reward, done
            next_state, reward, done = env.step(action)
            # print(reward)
            # 存经验值
            agent.appendMemory(state, action, reward, next_state, done)
 
            # agent.memory至少要收集agent.train_start（64）个才能开始训练
            # agent.global_step 没有达到 agent.target_update之前要用到target网络的地方由eval代替
            if agent.global_step >= agent.train_start:
                if agent.global_step <= agent.target_update:
                    agent.trainModel()
                else:
                    agent.trainModel(True)
            # 将回报值累加成score
            score += reward
 
            state = next_state
            # 发布 get_action 话题
            get_action.data = [action, score, reward]
            pub_get_action.publish(get_action)            
 
            # 超过500步时设定为超时，回合结束
            if t >= 500:
                print("Time out!!")
                done = True
 
            if done:
                # 发布result话题
                result.data = [score, np.max(agent.q_value)]
                pub_result.publish(result)
 
                scores.append(score)
                episodes.append(e)
                # 计算运行时间
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)
 
                rospy.loginfo('Ep: %d score: %.2f global_step: %d epsilon: %.2f time: %d:%02d:%02d',e, score, agent.global_step, agent.epsilon, h, m, s)
 
                break
 
            agent.global_step += 1
 
        # 更新衰减epsilon值，直到低于等于agent.epsilon_min
        if agent.epsilon > agent.epsilon_min:
            agent.epsilon *= agent.epsilon_decay

其中一次运行结果（double dqn）如下：


gazebo 报错 [Err] [REST.cc:205] Error in REST request 解决
具体现象如下：

修改：.ignition/fuel/config.yaml 
ROS:~$ gedit .ignition/fuel/config.yaml
将其改正即可

问题得解！

1.启动摄像头保存ros机器人在运动过程中拍摄的视频
在终端上输入:rosrun image_view video_recorder image:=/camera/rgb/image_raw

3、rgbdslam
1）安装rgbdslam环境
下载安装编译之前安装一个依赖包，命令如下：
	$ sudo apt-get install libsuitesparse-dev
 	$ sudo apt-get install libglew1.6-dev
	下载安装脚本：		
	wget -P ~/Downloads https://raw.githubusercontent.com/felixendres/rgbdslam_v2/kinetic/install.sh
下载后设置可执行权限，然后运行脚本：
$ cd ~/Downloads
	$ chmod +x install
	$ bash install.sh
完成编译后会在当前目录中生成一个Code文件夹，里面包含编译好的rgbslam功能包，运行前需要设置环境变量：
	$ source ~/Code/rgbdslam_catkin_ws/devel/setup.bash
		在Code文件夹中还有一个目录g2ofork，进入该目录，执行下面的命令安装g2o:
	$ mkdir build
	$ cd build
	$ cmake ..
	$ make
	$ sudo make install
下载pcl 1.8.0，编译安装pcl1.8源码：
	$ cd 到某个路径,存放pcl 
	$ wget  https://github.com/PointCloudLibrary/pcl/archive/pcl-1.8.0.tar.gz 
	$ tar -zxvf pcl-1.8.0.tar.gz 
	$ cd pcl-pcl-1.8.0/ 
	$ vim CMakeLists.txt 
	在146行加插入 SET(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11") 
	$ mkdir build&&cd build 
	$ cmake .. 
	$ make VERBOSE=1
 	$ sudo make install
接下来做以下操作:
修改rgbdslam_v2:  修改第 79 行:find_package(PCL 1.7 REQUIRED COMPONENTS common io) 为 find_package(PCL 1.8 REQUIRED COMPONENTS common io)
修改 /opt/ros/kinetic/share/pcl_ros/cmake/pcl_rosConfig.cmake文件,将所有/usr/lib/x86_64-linux_gnu/libpcl开头的内容改成/usr/local/lib/libpcl
	修改完成之后重新source install.sh 编译一下.
再次运行roslaunch rgbdslam rgbdslam.launch会出现以下界面(这就说明你已经安装成功了)

2）使用数据包实现SLAM
rgbdslam进行3D SLAM不需要机器人的里程计信息，只需要点云输 入就可以完成三维地图的构建。如果没有类似于Kinect的RGB-D 传感器，也可以使用数据包运行rgbdslam例程。 
数据包的下载命令如下： 
	$ wget -P ~/Downloads http://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk.bag
下载完成后，需要修改rgbdslam节点启动文件rgbdslam. Launch中的图像话题名，必须与数据包发布的话题一致。
也就是将下面的代码：

	改为：
	<param name="config/topic_image_mono" value="/camera/rgb/image_color" />
	<param name="config/topic_image_depth" value="/camera/depth/image" />
然后使用下面的命令运行rgbdslam的例程：
	$ roslaunch rgbdslam rgbdslam.launch
$ rosbag play rgbd_dataset_freiburg1_desk.bag 
	rgbdslam节点运行后会弹出一个可视化界面，当数据包开始发布数据时，就可以在界面中看到图像数据和SLAM的过程。
	如下图所示。

建图完成后，直接在菜单栏中选择保存为点云数据即可。可以使用pcl_ros功能包查看已保存的点云地图：
		$ rosrun pcl_ros pcd_to_pointcloud quicksave.pcd



